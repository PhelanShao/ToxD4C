#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EPA ToxCast/tcpl Compliant Toxicity Labeling System

This module implements a fully compliant EPA ToxCast/tcpl pipeline for generating
chemical toxicity labels from high-throughput screening data.

Key compliance features:
1. Hit call determination: Uses SC2 hitc values {-1,0,1}, excludes hitc=-1
2. Cytotoxicity filtering: cytotox_lower_bound_log with Î”=3 log10 units
3. Artifact control: mc6_flags for platform/multi-channel artifacts
4. Statistical modeling: Jeffreys Beta-Binomial with mechanism-weighted aggregation
5. External validation: ToxRefDB POD anchoring with nested cross-validation
6. Threshold selection: 5x3 nested CV with Youden's J optimization
"""

import pandas as pd
import numpy as np
import json
import hashlib
from scipy import stats
from sklearn.metrics import roc_auc_score
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class TcplSystemFullyCompliant:
    """EPA ToxCast/tcpl compliant toxicity labeling system"""

    def __init__(self):
        self.original_data = None
        self.sc2_data = None
        self.mc56_data = None
        self.cytotox_data = None
        self.assay_annotations = None
        self.bridge_table = None
        self.cas_pubchem_mapping = None
        self.toxrefdb_data = None
        self.mechanism_mapping = None
        self.chemical_scores = None
        self.final_labeled_data = None
        self.file_hashes = {}

    def calculate_file_sha256(self, file_path):
        """Calculate SHA256 hash for file integrity verification"""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            print(f"Warning: SHA256 calculation failed for {file_path}: {e}")
            return None
        
    def load_all_data_sources(self):
        """Load all required data sources for tcpl analysis"""
        print("Loading data sources...")

        # Load original 21st Century dataset
        try:
            self.original_data = pd.read_csv('output/data/processed_final8k213.csv')
            print(f"21st dataset: {len(self.original_data):,} records")
        except Exception as e:
            print(f"Error loading 21st dataset: {e}")
            return False
        
        # Load SC2 data (official source for hitc values)
        try:
            sc2_file = 'data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/sc1_sc2_invitrodb_v4_2_SEPT2024.xlsx'

            # Calculate SHA256 for integrity verification
            sc2_hash = self.calculate_file_sha256(sc2_file)
            self.file_hashes['sc2'] = sc2_hash

            self.sc2_data = pd.read_excel(sc2_file, sheet_name='sc2')

            # Validate hitc values are in {-1,0,1}
            valid_hitc = self.sc2_data['hitc'].isin([-1, 0, 1])
            self.sc2_data = self.sc2_data[valid_hitc]

            print(f"SC2 data loaded: {len(self.sc2_data):,} records")
            hitc_dist = dict(self.sc2_data['hitc'].value_counts().sort_index())
            print(f"hitc distribution: {hitc_dist}")

        except Exception as e:
            print(f"Error loading SC2 data: {e}")
            return False
        
        # 3. åŠ è½½cytotoxæ•°æ®ï¼ˆä½¿ç”¨æ­£ç¡®å­—æ®µåï¼‰
        try:
            cytotox_file = 'data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/cytotox_invitrodb_v4_2_SEPT2024.xlsx'

            # è®¡ç®—SHA256
            print("è®¡ç®—Cytotoxæ–‡ä»¶SHA256...")
            cytotox_hash = self.calculate_file_sha256(cytotox_file)
            self.file_hashes['cytotox'] = cytotox_hash
            print(f"   Cytotox SHA256: {cytotox_hash}")

            self.cytotox_data = pd.read_excel(cytotox_file)

            # æ£€æŸ¥æ­£ç¡®çš„å­—æ®µå
            cytotox_cols = [col for col in self.cytotox_data.columns if 'cytotox_lower_bound_log' in col.lower()]
            if cytotox_cols:
                print(f"âœ… Cytotoxæ•°æ®: {len(self.cytotox_data):,} è®°å½•")
                print(f"   ä½¿ç”¨å­—æ®µ: {cytotox_cols[0]}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°cytotox_lower_bound_logå­—æ®µ")
                print(f"   å¯ç”¨å­—æ®µ: {[col for col in self.cytotox_data.columns if 'cytotox' in col.lower()]}")

        except Exception as e:
            print(f"âŒ Cytotoxæ•°æ®åŠ è½½å¤±è´¥: {e}")
            self.cytotox_data = None

        # 4. åŠ è½½assay annotationsï¼ˆç”¨äºæœºåˆ¶æ˜ å°„ï¼‰
        try:
            assay_file = 'data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/assay_annotations_invitrodb_v4_2_SEPT2024.xlsx'

            # è®¡ç®—SHA256
            print("è®¡ç®—Assay annotationsæ–‡ä»¶SHA256...")
            assay_hash = self.calculate_file_sha256(assay_file)
            self.file_hashes['assay_annotations'] = assay_hash
            print(f"   Assay annotations SHA256: {assay_hash}")

            self.assay_annotations = pd.read_excel(assay_file)
            print(f"âœ… Assay annotations: {len(self.assay_annotations):,} è®°å½•")
        except Exception as e:
            print(f"âŒ Assay annotationsåŠ è½½å¤±è´¥: {e}")
            self.assay_annotations = None

        # 5. åŠ è½½æ˜ å°„æ•°æ®
        try:
            # CASâ†’PUBCHEM_CIDæ˜ å°„
            cas_file = 'output/data/cas_download_progress.json'
            cas_hash = self.calculate_file_sha256(cas_file)
            self.file_hashes['cas_mapping'] = cas_hash
            print(f"   CASæ˜ å°„ SHA256: {cas_hash}")

            with open(cas_file, 'r') as f:
                cas_data = json.load(f)
            self.cas_pubchem_mapping = cas_data['results']

            # chidâ†’CASæ¡¥è¡¨
            bridge_file = 'output/data/chemical_bridge_table.csv'
            bridge_hash = self.calculate_file_sha256(bridge_file)
            self.file_hashes['bridge_table'] = bridge_hash
            print(f"   æ¡¥è¡¨ SHA256: {bridge_hash}")

            self.bridge_table = pd.read_csv(bridge_file)

            # ToxRefDBæ•°æ®
            toxref_file = 'tox21_toxrefdb_matched_via_cas.csv'
            toxref_hash = self.calculate_file_sha256(toxref_file)
            self.file_hashes['toxrefdb'] = toxref_hash
            print(f"   ToxRefDB SHA256: {toxref_hash}")

            self.toxrefdb_data = pd.read_csv(toxref_file)

            print(f"âœ… æ˜ å°„æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   CASâ†’PUBCHEM_CID: {len(self.cas_pubchem_mapping):,} ä¸ª")
            print(f"   chidâ†’CASæ¡¥è¡¨: {len(self.bridge_table):,} ä¸ª")
            print(f"   ToxRefDB: {len(self.toxrefdb_data):,} ä¸ª")

        except Exception as e:
            print(f"âŒ æ˜ å°„æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
        
        return True
    
    def create_mechanism_mapping(self):
        """åˆ›å»ºç«¯ç‚¹åˆ°æœºåˆ¶çš„æ˜ å°„"""
        print("\nğŸ”§ åˆ›å»ºç«¯ç‚¹â†’æœºåˆ¶æ˜ å°„")
        print("-" * 50)
        
        if self.assay_annotations is None:
            print("âŒ ç¼ºå°‘assay annotationsï¼Œæ— æ³•åˆ›å»ºæœºåˆ¶æ˜ å°„")
            return False
        
        # æŸ¥æ‰¾æœºåˆ¶ç›¸å…³åˆ—
        mechanism_cols = []
        for col in self.assay_annotations.columns:
            if any(keyword in col.lower() for keyword in ['intended_target', 'biological_process', 'pathway']):
                mechanism_cols.append(col)
        
        if not mechanism_cols:
            print("âš ï¸ æœªæ‰¾åˆ°æœºåˆ¶åˆ—ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„")
            unique_aeids = self.sc2_data['aeid'].unique()
            self.mechanism_mapping = pd.DataFrame({
                'aeid': unique_aeids,
                'mechanism': 'GENERAL'
            })
            return True
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æœºåˆ¶åˆ—
        mechanism_col = mechanism_cols[0]
        print(f"ä½¿ç”¨æœºåˆ¶åˆ—: {mechanism_col}")
        
        # åˆ›å»ºæœºåˆ¶æ˜ å°„
        mechanism_data = self.assay_annotations[['aeid', mechanism_col]].copy()
        mechanism_data['mechanism_simplified'] = mechanism_data[mechanism_col].astype(str).str.upper()
        
        # æ”¹è¿›çš„æœºåˆ¶åˆ†ç±»ï¼ˆæ›´ç²¾ç»†ï¼‰
        def classify_mechanism(text):
            text = str(text).upper()

            # Nuclear Receptors (æ›´å…¨é¢çš„å…³é”®è¯)
            if any(keyword in text for keyword in [
                'NUCLEAR', 'RECEPTOR', 'HORMONE', 'ESTROGEN', 'ANDROGEN', 'THYROID',
                'GLUCOCORTICOID', 'MINERALOCORTICOID', 'PROGESTERONE', 'RETINOIC',
                'VITAMIN_D', 'PEROXISOME', 'PPAR', 'LXR', 'FXR', 'CAR', 'PXR'
            ]):
                return 'NR'

            # Stress Response (æ‰©å±•)
            elif any(keyword in text for keyword in [
                'STRESS', 'OXIDATIVE', 'ANTIOXIDANT', 'NRF2', 'KEAP1', 'ARE',
                'HEAT_SHOCK', 'HSP', 'UNFOLDED_PROTEIN', 'ER_STRESS'
            ]):
                return 'SR'

            # DNA Damage Response (æ‰©å±•)
            elif any(keyword in text for keyword in [
                'DNA', 'GENOTOX', 'P53', 'ATM', 'ATR', 'REPAIR', 'CHECKPOINT',
                'BRCA', 'PARP', 'HOMOLOGOUS', 'NHEJ', 'MUTAGENIC'
            ]):
                return 'DDR'

            # Cytotoxicity (æ‰©å±•)
            elif any(keyword in text for keyword in [
                'CYTOTOX', 'CELL_DEATH', 'VIABILITY', 'MITOCHONDRIA', 'APOPTOSIS',
                'NECROSIS', 'AUTOPHAGY', 'MEMBRANE', 'ATP', 'RESPIRATION'
            ]):
                return 'CYTO'

            # Metabolism (æ–°å¢)
            elif any(keyword in text for keyword in [
                'METABOLISM', 'METABOLIC', 'CYP', 'CYTOCHROME', 'PHASE_I', 'PHASE_II',
                'GLUCURONIDATION', 'SULFATION', 'ACETYLATION', 'METHYLATION'
            ]):
                return 'MET'

            # Neurotoxicity (æ–°å¢)
            elif any(keyword in text for keyword in [
                'NEURO', 'NEURAL', 'SYNAPSE', 'NEUROTRANSMITTER', 'ACETYLCHOLINE',
                'DOPAMINE', 'SEROTONIN', 'GABA', 'GLUTAMATE', 'CALCIUM_CHANNEL'
            ]):
                return 'NEURO'

            else:
                return 'GENERAL'
        
        mechanism_data['mechanism'] = mechanism_data['mechanism_simplified'].apply(classify_mechanism)
        
        self.mechanism_mapping = mechanism_data[['aeid', 'mechanism']].drop_duplicates()
        
        mechanism_dist = self.mechanism_mapping['mechanism'].value_counts()
        print(f"æœºåˆ¶åˆ†å¸ƒ: {dict(mechanism_dist)}")

        return True

    def load_mc56_data_full_traversal(self):
        """å…¨é‡éå†åŠ è½½MC5-6æ•°æ®ï¼ˆä»…ç”¨äºflagså’Œac50ï¼‰"""
        print("\nğŸ“š å…¨é‡éå†MC5-6æ•°æ®")
        print("-" * 50)

        mc56_file = 'data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/mc5-6_winning_model_fits-flags_invitrodbv4_2_SEPT2024.csv'

        # è·å–SC2ä¸­çš„(chid, aeid)ç»„åˆç”¨äºè¿‡æ»¤
        sc2_keys = set(zip(self.sc2_data['chid'].astype(str), self.sc2_data['aeid'].astype(str)))
        print(f"SC2ä¸­çš„(chid,aeid)ç»„åˆ: {len(sc2_keys):,} ä¸ª")

        mc56_chunks = []
        chunk_count = 0
        total_processed = 0

        print("å¼€å§‹å…¨é‡éå†MC5-6æ–‡ä»¶...")

        # å…¨é‡éå†ï¼Œç»ä¸æ—©åœ
        for chunk in pd.read_csv(mc56_file, chunksize=100000):
            chunk_count += 1
            total_processed += len(chunk)

            # æ¯å—å†…å…ˆæŒ‰(chid, aeid)è¿‡æ»¤
            chunk_keys = set(zip(chunk['chid'].astype(str), chunk['aeid'].astype(str)))
            matching_keys = chunk_keys.intersection(sc2_keys)

            if matching_keys:
                chunk_mask = chunk.apply(
                    lambda row: (str(row['chid']), str(row['aeid'])) in matching_keys,
                    axis=1
                )
                chunk_filtered = chunk[chunk_mask]

                if len(chunk_filtered) > 0:
                    # åªä¿ç•™éœ€è¦çš„åˆ—ï¼šmc6_flags, ac50
                    essential_cols = ['chid', 'aeid', 'mc6_flags', 'ac50']
                    chunk_filtered = chunk_filtered[essential_cols].copy()
                    mc56_chunks.append(chunk_filtered)

            if chunk_count % 50 == 0:
                print(f"   å·²å¤„ç† {chunk_count} ä¸ªå—ï¼Œæ€»è®°å½• {total_processed:,}")

        # åˆå¹¶æ‰€æœ‰å—
        self.mc56_data = pd.concat(mc56_chunks, ignore_index=True) if mc56_chunks else pd.DataFrame()

        print(f"âœ… MC5-6æ•°æ®å…¨é‡éå†å®Œæˆ:")
        print(f"   å¤„ç†å—æ•°: {chunk_count}")
        print(f"   æ€»å¤„ç†è®°å½•: {total_processed:,}")
        print(f"   åŒ¹é…è®°å½•: {len(self.mc56_data):,}")

        return True

    def apply_tcpl_compliant_filtering_with_cytotox(self):
        """åº”ç”¨tcplåˆè§„è¿‡æ»¤ï¼ˆåŒ…å«æ­£ç¡®çš„ç»†èƒæ¯’æ§åˆ¶ï¼‰"""
        print("\nğŸ”§ åº”ç”¨tcplåˆè§„è¿‡æ»¤")
        print("-" * 50)

        # åˆå¹¶SC2ï¼ˆhitcæ¥æºï¼‰å’ŒMC5-6ï¼ˆflags/ac50æ¥æºï¼‰
        print("åˆå¹¶SC2å’ŒMC5-6æ•°æ®...")

        if len(self.mc56_data) > 0:
            merged_data = self.sc2_data.merge(
                self.mc56_data,
                on=['chid', 'aeid'],
                how='left'  # å·¦è¿æ¥ï¼Œä»¥SC2ä¸ºå‡†
            )
        else:
            merged_data = self.sc2_data.copy()
            merged_data['mc6_flags'] = ''
            merged_data['ac50'] = np.nan

        print(f"åˆå¹¶åæ•°æ®: {len(merged_data):,} è®°å½•")

        # 1. åˆ†æ¯å£å¾„ä¿®æ­£ï¼šæ’é™¤hitc=-1ï¼ˆæœªå®š/ä¸é€‚ç”¨ï¼‰
        print("æ’é™¤hitc=-1è®°å½•ï¼ˆæœªå®š/ä¸é€‚ç”¨ï¼‰...")
        before_filter = len(merged_data)
        merged_data = merged_data[merged_data['hitc'] != -1]
        after_filter = len(merged_data)
        print(f"æ’é™¤hitc=-1å: {after_filter:,} è®°å½• (æ’é™¤äº† {before_filter-after_filter:,} æ¡)")

        # 2. åˆ†æ¯/é˜³æ€§åˆ¤å®šï¼štested=è®°å½•å­˜åœ¨ä¸”hitcâ‰ -1ï¼Œé˜³æ€§=hitc==1
        merged_data['tested'] = 1
        merged_data['positive'] = (merged_data['hitc'] == 1).astype(int)

        print(f"Testedè®°å½•: {merged_data['tested'].sum():,}")
        print(f"é˜³æ€§è®°å½• (hitc==1): {merged_data['positive'].sum():,}")

        # 2. ä¼ªå½±æ§åˆ¶ï¼šä½¿ç”¨mc6_flags
        has_flags = merged_data['mc6_flags'].notna() & (merged_data['mc6_flags'] != '')
        merged_data['artifact_flag'] = has_flags.astype(int)
        print(f"æœ‰mc6_flagsçš„è®°å½•: {has_flags.sum():,}")

        # 3. ç»†èƒæ¯’æ§åˆ¶ï¼šä½¿ç”¨cytotox_lower_bound_logï¼ŒÎ”=3 log10è·ç¦»
        merged_data['cytotox_flag'] = 0

        if self.cytotox_data is not None:
            # æŸ¥æ‰¾æ­£ç¡®çš„cytotoxå­—æ®µ
            cytotox_col = None
            for col in self.cytotox_data.columns:
                if 'cytotox_lower_bound_log' in col.lower():
                    cytotox_col = col
                    break

            if cytotox_col:
                print(f"ä½¿ç”¨cytotoxå­—æ®µ: {cytotox_col}")

                # åˆ›å»ºchidåˆ°cytotoxé˜ˆå€¼çš„æ˜ å°„ï¼ˆä¿®æ­£ç±»å‹åŒ¹é…ï¼‰
                cytotox_mapping = {}
                for _, row in self.cytotox_data.iterrows():
                    chid = str(int(row['chid'])) if pd.notna(row['chid']) else None
                    if chid and pd.notna(row.get(cytotox_col)):
                        cytotox_mapping[chid] = row[cytotox_col]

                print(f"Cytotoxæ˜ å°„: {len(cytotox_mapping):,} ä¸ªåŒ–å­¦å“")

                # ä¿®æ­£ac50æ•°æ®å¤„ç†å’Œå•ä½ç»Ÿä¸€
                print("å¤„ç†ac50æ•°æ®å’Œå•ä½ç»Ÿä¸€...")

                # 1. å¤„ç†ac50æ•°å€¼
                ac50_raw = pd.to_numeric(merged_data['ac50'], errors='coerce')
                ac50_raw = ac50_raw.replace([np.inf, -np.inf], np.nan)

                # 2. å•ä½æ ‡å‡†åŒ–æ ¸éªŒå’Œè½¬æ¢åˆ°Î¼M
                print("  å•ä½æ ‡å‡†åŒ–æ ¸éªŒ...")

                if 'conc_unit' in merged_data.columns:
                    unit_dist = merged_data['conc_unit'].value_counts()
                    print(f"  æµ“åº¦å•ä½åˆ†å¸ƒ: {dict(unit_dist)}")

                    # å•ä½è½¬æ¢åˆ°Î¼Mï¼ˆè¯¦ç»†æ¢ç®—è§„åˆ™ï¼‰
                    ac50_um = ac50_raw.copy()
                    conversion_log = {}

                    for unit in unit_dist.index:
                        mask = merged_data['conc_unit'] == unit
                        count = mask.sum()

                        if unit in ['uM', 'Î¼M', 'micromolar']:
                            # å·²ç»æ˜¯Î¼Mï¼Œæ— éœ€è½¬æ¢
                            conversion_factor = 1.0
                        elif unit in ['nM', 'nanomolar']:
                            # nM â†’ Î¼M: é™¤ä»¥1000
                            conversion_factor = 1.0 / 1000
                            ac50_um.loc[mask] = ac50_raw.loc[mask] * conversion_factor
                        elif unit in ['mM', 'millimolar']:
                            # mM â†’ Î¼M: ä¹˜ä»¥1000
                            conversion_factor = 1000.0
                            ac50_um.loc[mask] = ac50_raw.loc[mask] * conversion_factor
                        elif unit in ['M', 'molar']:
                            # M â†’ Î¼M: ä¹˜ä»¥1e6
                            conversion_factor = 1e6
                            ac50_um.loc[mask] = ac50_raw.loc[mask] * conversion_factor
                        elif unit in ['mg/l', 'mg/L']:
                            # mg/léœ€è¦åˆ†å­é‡ï¼Œæš‚æ—¶æ ‡è®°ä¸ºç¼ºå¤±
                            conversion_factor = None
                            ac50_um.loc[mask] = np.nan
                            print(f"    âš ï¸ {unit}: {count}ä¸ªè®°å½•éœ€è¦åˆ†å­é‡è½¬æ¢ï¼Œæš‚æ—¶è®¾ä¸ºNaN")
                        else:
                            # æœªçŸ¥å•ä½ï¼Œè®¾ä¸ºç¼ºå¤±
                            conversion_factor = None
                            ac50_um.loc[mask] = np.nan
                            print(f"    âš ï¸ æœªçŸ¥å•ä½{unit}: {count}ä¸ªè®°å½•è®¾ä¸ºNaN")

                        conversion_log[unit] = {
                            'count': count,
                            'factor': conversion_factor,
                            'description': f"{unit} â†’ Î¼M"
                        }

                    # ä¿å­˜è½¬æ¢æ—¥å¿—
                    self.ac50_conversion_log = conversion_log

                    # è½¬æ¢å‰åç»Ÿè®¡
                    print(f"  è½¬æ¢å‰ac50ç»Ÿè®¡: éç©º{(ac50_raw > 0).sum():,}, èŒƒå›´{ac50_raw.min():.2e}-{ac50_raw.max():.2e}")
                    print(f"  è½¬æ¢åac50ç»Ÿè®¡: éç©º{(ac50_um > 0).sum():,}, èŒƒå›´{ac50_um.min():.2e}-{ac50_um.max():.2e}")

                else:
                    # å‡è®¾éƒ½æ˜¯Î¼M
                    ac50_um = ac50_raw.copy()
                    print("  âš ï¸ ç¼ºå°‘conc_unitåˆ—ï¼Œå‡è®¾æ‰€æœ‰ac50éƒ½æ˜¯Î¼Må•ä½")
                    self.ac50_conversion_log = {'assumed_uM': {'count': len(ac50_raw), 'factor': 1.0}}

                # 3. è®¡ç®—log10(ac50_Î¼M)
                log_ac50_um = np.log10(ac50_um.where(ac50_um > 0))

                # 4. åˆ›å»ºcytotoxé˜ˆå€¼æ˜ å°„ï¼ˆä¿®æ­£chidç±»å‹åŒ¹é…ï¼‰
                merged_data['cyto_lb_log10'] = np.nan
                for idx, row in merged_data.iterrows():
                    chid_str = str(int(row['chid'])) if pd.notna(row['chid']) else None
                    if chid_str and chid_str in cytotox_mapping:
                        merged_data.loc[idx, 'cyto_lb_log10'] = cytotox_mapping[chid_str]

                # 5. ç»Ÿè®¡æ•°æ®è´¨é‡
                n_ac50_pos = (ac50_um > 0).sum()
                n_with_cyto = merged_data['cyto_lb_log10'].notna().sum()
                n_overlap = ((ac50_um > 0) & merged_data['cyto_lb_log10'].notna()).sum()

                print(f"æ•°æ®è´¨é‡ç»Ÿè®¡:")
                print(f"  æœ‰æ•ˆac50 (>0): {n_ac50_pos:,}")
                print(f"  æœ‰cytotoxé˜ˆå€¼: {n_with_cyto:,}")
                print(f"  ä¸¤è€…äº¤é›†: {n_overlap:,}")

                # 6. å‘é‡åŒ–cytotoxåˆ¤å®šï¼ˆÎ”=3 log10å•ä½ï¼‰
                merged_data['cytotox_flag'] = (
                    (ac50_um > 0) &
                    merged_data['cyto_lb_log10'].notna() &
                    (log_ac50_um >= (merged_data['cyto_lb_log10'] - 3))
                ).astype(int)

                cytotox_flagged = merged_data['cytotox_flag'].sum()
                print(f"Cytotoxæ ‡è®°çš„è®°å½•: {cytotox_flagged:,}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°cytotox_lower_bound_logå­—æ®µ")

        self.merged_data = merged_data
        return merged_data

    def calculate_mechanism_weighted_scores(self):
        """è®¡ç®—æœºåˆ¶ç­‰æƒBeta-Binomialåˆ†æ•°"""
        print("\nğŸ§® è®¡ç®—æœºåˆ¶ç­‰æƒBeta-Binomialåˆ†æ•°")
        print("-" * 50)

        # æ·»åŠ æœºåˆ¶ä¿¡æ¯
        data_with_mechanism = self.merged_data.merge(
            self.mechanism_mapping,
            on='aeid',
            how='left'
        )

        # å¡«å……ç¼ºå¤±æœºåˆ¶
        data_with_mechanism['mechanism'] = data_with_mechanism['mechanism'].fillna('GENERAL')

        print(f"æ•°æ®ä¸æœºåˆ¶æ˜ å°„å®Œæˆ: {len(data_with_mechanism):,} è®°å½•")

        # åº”ç”¨è´¨é‡æ§åˆ¶è¿‡æ»¤
        clean_data = data_with_mechanism[
            (data_with_mechanism['artifact_flag'] == 0) &
            (data_with_mechanism['cytotox_flag'] == 0)
        ]

        print(f"è´¨é‡æ§åˆ¶å: {len(clean_data):,} è®°å½•")

        # æŒ‰åŒ–å­¦å“è®¡ç®—æœºåˆ¶ç­‰æƒåˆ†æ•°
        chemical_stats = []
        alpha, beta = 0.5, 0.5  # Jeffreyså…ˆéªŒ

        for chid in clean_data['chid'].unique():
            chid_data = clean_data[clean_data['chid'] == chid]

            # æŒ‰æœºåˆ¶åˆ†ç»„è®¡ç®—
            mechanism_scores = []
            mechanism_details = {}

            for mechanism in chid_data['mechanism'].unique():
                mech_data = chid_data[chid_data['mechanism'] == mechanism]

                n_tested = len(mech_data)
                n_positive = mech_data['positive'].sum()

                # Beta-Binomialæ”¶ç¼©
                posterior_alpha = n_positive + alpha
                posterior_beta = n_tested - n_positive + beta
                p_shrunk = posterior_alpha / (posterior_alpha + posterior_beta)

                mechanism_scores.append(p_shrunk)
                mechanism_details[mechanism] = {
                    'n_tested': n_tested,
                    'n_positive': n_positive,
                    'score': p_shrunk
                }

            # æœºåˆ¶ç­‰æƒå¹³å‡
            S_c = np.mean(mechanism_scores) if mechanism_scores else 0

            # æœºåˆ¶ç­‰æƒçš„ç½®ä¿¡åŒºé—´ï¼ˆæœºåˆ¶å±‚bootstrapï¼‰
            if len(mechanism_scores) > 1:
                # å¯¹æœºåˆ¶é›†åˆè¿›è¡Œbootstrapé‡é‡‡æ ·
                n_bootstrap = 1000
                bootstrap_scores = []
                np.random.seed(42)  # å›ºå®šç§å­

                for _ in range(n_bootstrap):
                    # é‡é‡‡æ ·æœºåˆ¶
                    boot_mechanisms = np.random.choice(len(mechanism_scores), len(mechanism_scores), replace=True)
                    boot_scores = [mechanism_scores[i] for i in boot_mechanisms]
                    boot_S_c = np.mean(boot_scores)
                    bootstrap_scores.append(boot_S_c)

                ci_lower = np.percentile(bootstrap_scores, 2.5)
                ci_upper = np.percentile(bootstrap_scores, 97.5)
            else:
                # å•æœºåˆ¶æƒ…å†µï¼Œä½¿ç”¨Betaåˆ†å¸ƒCI
                total_tested = len(chid_data)
                total_positive = chid_data['positive'].sum()
                total_posterior_alpha = total_positive + alpha
                total_posterior_beta = total_tested - total_positive + beta
                ci_lower = stats.beta.ppf(0.025, total_posterior_alpha, total_posterior_beta)
                ci_upper = stats.beta.ppf(0.975, total_posterior_alpha, total_posterior_beta)

            chemical_stats.append({
                'chid': chid,
                'n_tested': total_tested,
                'n_positive': total_positive,
                'S_c': S_c,
                'ci_lower': ci_lower,
                'ci_upper': ci_upper,
                'raw_rate': total_positive / total_tested if total_tested > 0 else 0,
                'n_mechanisms': len(mechanism_scores),
                'mechanism_details': mechanism_details
            })

        self.chemical_scores = pd.DataFrame(chemical_stats)

        print(f"æœºåˆ¶ç­‰æƒåˆ†æ•°è®¡ç®—å®Œæˆ:")
        print(f"  åŒ–å­¦å“æ•°: {len(self.chemical_scores):,}")
        print(f"  S_cèŒƒå›´: {self.chemical_scores['S_c'].min():.6f} - {self.chemical_scores['S_c'].max():.6f}")
        print(f"  å¹³å‡æµ‹è¯•æ•°: {self.chemical_scores['n_tested'].mean():.1f}")
        print(f"  å¹³å‡æœºåˆ¶æ•°: {self.chemical_scores['n_mechanisms'].mean():.1f}")

        return self.chemical_scores

    def external_validation_correct_mapping(self):
        """å¤–éƒ¨éªŒè¯ï¼ˆä½¿ç”¨æ­£ç¡®æ˜ å°„é“¾è·¯ï¼‰"""
        print("\nğŸ¯ å¤–éƒ¨éªŒè¯ï¼ˆæ­£ç¡®æ˜ å°„é“¾è·¯ï¼‰")
        print("-" * 50)

        # ä½¿ç”¨SC2çš„casnåšåº•åº§
        chid_to_casn = {}
        for _, row in self.sc2_data[['chid', 'casn']].drop_duplicates().iterrows():
            if pd.notna(row['casn']) and row['casn'] != '':
                chid_to_casn[row['chid']] = str(row['casn']).strip()

        print(f"SC2ä¸­æœ‰CASå·çš„chid: {len(chid_to_casn):,}")

        # ä¸ºåŒ–å­¦å“åˆ†æ•°æ·»åŠ CASä¿¡æ¯
        scores_with_cas = self.chemical_scores.copy()
        scores_with_cas['casn'] = scores_with_cas['chid'].map(chid_to_casn)
        scores_with_cas = scores_with_cas.dropna(subset=['casn'])

        print(f"æœ‰CASå·çš„åŒ–å­¦å“åˆ†æ•°: {len(scores_with_cas):,}")

        # ç¨³å¥å¤–éƒ¨é”šå®šï¼šè¿‡æ»¤ToxRefDBæ•°æ®è´¨é‡
        print("åº”ç”¨ç¨³å¥å¤–éƒ¨é”šå®šè¿‡æ»¤...")

        # è¿‡æ»¤æ¡ä»¶ï¼šPODåœ¨åˆç†èŒƒå›´å†…ï¼Œæ’é™¤æç«¯å€¼
        toxref_filtered = self.toxrefdb_data[
            (self.toxrefdb_data['POD_MGKGDAY'] >= 0.001) &  # æœ€å°0.001 mg/kg/day
            (self.toxrefdb_data['POD_MGKGDAY'] <= 10000) &  # æœ€å¤§10g/kg/day
            (self.toxrefdb_data['POD_MGKGDAY'].notna())
        ].copy()

        print(f"ToxRefDBè¿‡æ»¤: {len(self.toxrefdb_data):,} â†’ {len(toxref_filtered):,} è®°å½•")

        # ä¸è¿‡æ»¤åçš„ToxRefDBå¯¹é½ï¼šchidâ†’casnâ†’PUBCHEM_CID
        validation_data = scores_with_cas.merge(
            toxref_filtered[['CAS_NORM', 'POD_MGKGDAY', 'PUBCHEM_CID']],
            left_on='casn',
            right_on='CAS_NORM',
            how='inner'
        )

        print(f"éªŒè¯æ•°æ®é›†: {len(validation_data):,} ä¸ªåŒ–å­¦å“")
        print(f"å¯¹åº”PUBCHEM_CID: {validation_data['PUBCHEM_CID'].nunique():,} ä¸ª")

        if len(validation_data) >= 10:
            # Spearmanç›¸å…³æ€§
            spearman_r, spearman_p = stats.spearmanr(
                validation_data['S_c'],
                -np.log10(validation_data['POD_MGKGDAY'])
            )
            print(f"Spearman r(S_c vs -log10(POD)): {spearman_r:.3f}, p={spearman_p:.3e}")

            # å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡åˆ†æ
            from sklearn.metrics import precision_recall_curve, average_precision_score

            performance_results = {}

            for tau in [3, 10, 30]:
                y_true = (validation_data['POD_MGKGDAY'] <= tau).astype(int)
                y_scores = validation_data['S_c']

                if len(np.unique(y_true)) > 1:
                    # ROC-AUC
                    auc_score = roc_auc_score(y_true, y_scores)

                    # æ–¹å‘æ£€æŸ¥
                    if auc_score < 0.5:
                        print(f"âš ï¸ Ï„={tau}: AUC<0.5ï¼Œåº”ç”¨æ–¹å‘åè½¬")
                        y_scores_corrected = -y_scores
                        auc_score = roc_auc_score(y_true, y_scores_corrected)
                        direction = "reversed"
                    else:
                        y_scores_corrected = y_scores
                        direction = "normal"

                    # DeLong 95% CI (ç®€åŒ–ç‰ˆbootstrap)
                    n_bootstrap = 1000
                    bootstrap_aucs = []
                    np.random.seed(42)

                    for _ in range(n_bootstrap):
                        indices = np.random.choice(len(y_true), len(y_true), replace=True)
                        if len(np.unique(y_true[indices])) > 1:
                            boot_auc = roc_auc_score(y_true[indices], y_scores_corrected[indices])
                            bootstrap_aucs.append(boot_auc)

                    if bootstrap_aucs:
                        auc_ci_lower = np.percentile(bootstrap_aucs, 2.5)
                        auc_ci_upper = np.percentile(bootstrap_aucs, 97.5)
                    else:
                        auc_ci_lower = auc_ci_upper = auc_score

                    # PR-AUC
                    precision, recall, _ = precision_recall_curve(y_true, y_scores_corrected)
                    pr_auc = average_precision_score(y_true, y_scores_corrected)

                    # Brieråˆ†æ•°
                    # å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç®€åŒ–ï¼šä½¿ç”¨sigmoidå˜æ¢ï¼‰
                    from scipy.special import expit
                    y_prob = expit(y_scores_corrected)  # sigmoidå˜æ¢
                    brier_score = np.mean((y_prob - y_true) ** 2)

                    performance_results[tau] = {
                        'auc': auc_score,
                        'auc_ci': (auc_ci_lower, auc_ci_upper),
                        'pr_auc': pr_auc,
                        'brier_score': brier_score,
                        'direction': direction,
                        'n_pos': y_true.sum(),
                        'n_total': len(y_true)
                    }

                    print(f"AUC (Ï„={tau} mg/kg/day): {auc_score:.3f} "
                          f"[95%CI: {auc_ci_lower:.3f}-{auc_ci_upper:.3f}] "
                          f"({direction}), n_pos={y_true.sum()}")
                    print(f"  PR-AUC: {pr_auc:.3f}, Brier: {brier_score:.3f}")

            # ä¿å­˜æ€§èƒ½ç»“æœ
            self.performance_results = performance_results

        self.validation_data = validation_data
        return validation_data

    def create_labels_and_merge_to_original(self):
        """åˆ›å»ºæ ‡ç­¾å¹¶åˆå¹¶åˆ°åŸå§‹processed_final8k213.csv"""
        print("\nğŸ·ï¸ åˆ›å»ºæ ‡ç­¾å¹¶åˆå¹¶åˆ°åŸå§‹æ–‡ä»¶")
        print("-" * 50)

        # ä½¿ç”¨å¤–éƒ¨éªŒè¯ç¡®å®šé˜ˆå€¼
        if hasattr(self, 'validation_data') and len(self.validation_data) >= 10:
            # ä½¿ç”¨Ï„=10 mg/kg/dayä½œä¸ºä¸»è¦é”šå®šç‚¹
            tau = 10
            y_true = (self.validation_data['POD_MGKGDAY'] <= tau).astype(int)

            if len(np.unique(y_true)) > 1:
                from sklearn.metrics import roc_curve
                fpr, tpr, thresholds = roc_curve(y_true, self.validation_data['S_c'])
                youden_j = tpr - fpr
                best_idx = np.argmax(youden_j)
                binary_threshold = thresholds[best_idx]

                print(f"å¤–éƒ¨é”šå®šäºŒåˆ†ç±»é˜ˆå€¼ (Ï„={tau}): {binary_threshold:.6f}")
            else:
                binary_threshold = self.chemical_scores['S_c'].median()
        else:
            binary_threshold = self.chemical_scores['S_c'].median()

        # 5Ã—3åµŒå¥—äº¤å‰éªŒè¯é˜ˆå€¼é€‰æ‹©
        if hasattr(self, 'validation_data') and len(self.validation_data) >= 30:
            print("ä½¿ç”¨5Ã—3åµŒå¥—äº¤å‰éªŒè¯ç¡®å®šé˜ˆå€¼...")

            from sklearn.model_selection import StratifiedKFold
            from sklearn.metrics import roc_curve, f1_score, precision_recall_curve, average_precision_score

            # å‡†å¤‡éªŒè¯æ•°æ®ï¼ˆä½¿ç”¨NumPyæ•°ç»„é¿å…ç´¢å¼•é—®é¢˜ï¼‰
            S_scores = self.validation_data['S_c'].values

            # ä¸åŒÏ„å€¼çš„æ ‡ç­¾
            tau_values = {'binary': 10, 'ternary_low': 30, 'ternary_high': 3}
            nested_cv_results = {}

            for label_type, tau in tau_values.items():
                print(f"  {label_type} (Ï„={tau})...")
                y_labels = (self.validation_data['POD_MGKGDAY'] <= tau).astype(int).values

                if len(np.unique(y_labels)) < 2:
                    print(f"    âš ï¸ Ï„={tau}ç±»åˆ«ä¸å¹³è¡¡ï¼Œè·³è¿‡åµŒå¥—CV")
                    continue

                # å¤–å±‚5æŠ˜ï¼ˆåˆ†å±‚æŠ½æ ·ï¼‰
                outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
                outer_aucs = []
                outer_pr_aucs = []
                outer_thresholds = []

                for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(S_scores, y_labels)):
                    # ä½¿ç”¨NumPyåˆ‡ç‰‡
                    S_train, S_test = S_scores[train_idx], S_scores[test_idx]
                    y_train, y_test = y_labels[train_idx], y_labels[test_idx]

                    # å†…å±‚3æŠ˜é€‰æ‹©é˜ˆå€¼ï¼ˆä»…åœ¨è®­ç»ƒé›†ä¸Šï¼‰
                    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

                    # å€™é€‰é˜ˆå€¼ï¼šä½¿ç”¨è®­ç»ƒé›†åˆ†ä½æ•°
                    candidate_thresholds = np.percentile(S_train, [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95])

                    best_threshold = None
                    best_youden_j = -1

                    # å†…å±‚é˜ˆå€¼é€‰æ‹©è®°å½•
                    inner_fold_results = []

                    for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(inner_cv.split(S_train, y_train)):
                        S_inner_val = S_train[inner_val_idx]
                        y_inner_val = y_train[inner_val_idx]

                        # åœ¨å†…å±‚éªŒè¯é›†ä¸Šè¯„ä¼°å€™é€‰é˜ˆå€¼ï¼ˆä»…ç”¨Youden's Jï¼‰
                        if len(np.unique(y_inner_val)) > 1:
                            inner_best_youden = -1
                            inner_best_thresh = None

                            for thresh in candidate_thresholds:
                                y_pred_inner = (S_inner_val >= thresh).astype(int)

                                # åªè®¡ç®—Youden's Jï¼Œç»ä¸ç”¨AUCé€‰é˜ˆå€¼
                                if len(np.unique(y_pred_inner)) > 1:
                                    from sklearn.metrics import confusion_matrix
                                    try:
                                        tn, fp, fn, tp = confusion_matrix(y_inner_val, y_pred_inner).ravel()
                                        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
                                        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
                                        youden_j = sensitivity + specificity - 1

                                        if youden_j > inner_best_youden:
                                            inner_best_youden = youden_j
                                            inner_best_thresh = thresh
                                    except:
                                        continue

                            # è®°å½•å†…å±‚ç»“æœ
                            if inner_best_thresh is not None:
                                inner_fold_results.append({
                                    'inner_fold': inner_fold,
                                    'threshold': inner_best_thresh,
                                    'youden_j': inner_best_youden,
                                    'n_samples': len(y_inner_val),
                                    'n_positive': y_inner_val.sum()
                                })

                                # æ›´æ–°å…¨å±€æœ€ä½³
                                if inner_best_youden > best_youden_j:
                                    best_youden_j = inner_best_youden
                                    best_threshold = inner_best_thresh

                    # åœ¨å¤–å±‚æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆä¸å†è°ƒå‚ï¼‰
                    if best_threshold is not None and len(np.unique(y_test)) > 1:
                        test_auc = roc_auc_score(y_test, S_test)
                        test_pr_auc = average_precision_score(y_test, S_test)

                        # è®°å½•è¯¦ç»†çš„å¤–å±‚æŠ˜ä¿¡æ¯ï¼ˆå¯è¿½æº¯æ€§ï¼‰
                        outer_fold_record = {
                            'fold': fold_idx + 1,
                            'train_indices': train_idx.tolist(),
                            'test_indices': test_idx.tolist(),
                            'train_pubchem_cids': self.validation_data.iloc[train_idx]['PUBCHEM_CID'].tolist(),
                            'test_pubchem_cids': self.validation_data.iloc[test_idx]['PUBCHEM_CID'].tolist(),
                            'best_threshold': best_threshold,
                            'test_auc': test_auc,
                            'test_pr_auc': test_pr_auc,
                            'n_train': len(train_idx),
                            'n_test': len(test_idx),
                            'n_test_positive': y_test.sum(),
                            'inner_fold_details': inner_fold_results
                        }

                        if not hasattr(self, 'nested_cv_detailed_results'):
                            self.nested_cv_detailed_results = {}
                        if label_type not in self.nested_cv_detailed_results:
                            self.nested_cv_detailed_results[label_type] = []

                        self.nested_cv_detailed_results[label_type].append(outer_fold_record)

                        outer_aucs.append(test_auc)
                        outer_pr_aucs.append(test_pr_auc)
                        outer_thresholds.append(best_threshold)

                        print(f"    Fold {fold_idx+1}: AUC={test_auc:.3f}, PR-AUC={test_pr_auc:.3f}, Thresh={best_threshold:.4f}")

                # æ±‡æ€»å¤–å±‚5æŠ˜ç»“æœ
                if outer_thresholds:
                    mean_threshold = np.mean(outer_thresholds)
                    std_threshold = np.std(outer_thresholds)
                    mean_auc = np.mean(outer_aucs)
                    std_auc = np.std(outer_aucs)
                    mean_pr_auc = np.mean(outer_pr_aucs)
                    std_pr_auc = np.std(outer_pr_aucs)

                    # æ–¹å‘æ£€æŸ¥
                    direction = "normal"
                    if mean_auc < 0.5:
                        print(f"    âš ï¸ å¤–å±‚å¹³å‡AUC<0.5ï¼Œåº”ç”¨æ–¹å‘åè½¬")
                        # é‡æ–°è¯„ä¼°-S_c
                        corrected_aucs = []
                        for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(S_scores, y_labels)):
                            S_test = S_scores[test_idx]
                            y_test = y_labels[test_idx]
                            if len(np.unique(y_test)) > 1:
                                corrected_auc = roc_auc_score(y_test, -S_test)
                                corrected_aucs.append(corrected_auc)

                        if corrected_aucs:
                            mean_auc = np.mean(corrected_aucs)
                            std_auc = np.std(corrected_aucs)
                            direction = "reversed"

                    nested_cv_results[label_type] = {
                        'threshold': mean_threshold,
                        'threshold_std': std_threshold,
                        'auc': mean_auc,
                        'auc_std': std_auc,
                        'pr_auc': mean_pr_auc,
                        'pr_auc_std': std_pr_auc,
                        'direction': direction,
                        'n_folds': len(outer_thresholds)
                    }

                    print(f"    å¤–å±‚5æŠ˜æ±‡æ€»:")
                    print(f"      é˜ˆå€¼: {mean_threshold:.6f}Â±{std_threshold:.6f}")
                    print(f"      AUC: {mean_auc:.3f}Â±{std_auc:.3f} ({direction})")
                    print(f"      PR-AUC: {mean_pr_auc:.3f}Â±{std_pr_auc:.3f}")

            # ä½¿ç”¨åµŒå¥—CVç»“æœæˆ–å›é€€åˆ°ç®€å•æ–¹æ³•
            if 'binary' in nested_cv_results:
                binary_threshold = nested_cv_results['binary']['threshold']
                print(f"åµŒå¥—CVäºŒåˆ†ç±»é˜ˆå€¼: {binary_threshold:.6f}")
            else:
                # å›é€€åˆ°ç®€å•Youden's J
                tau = 10
                y_true = (self.validation_data['POD_MGKGDAY'] <= tau).astype(int)
                if len(np.unique(y_true)) > 1:
                    fpr, tpr, thresholds = roc_curve(y_true, self.validation_data['S_c'])
                    youden_j = tpr - fpr
                    best_idx = np.argmax(youden_j)
                    binary_threshold = thresholds[best_idx]
                    print(f"ç®€å•å¤–éƒ¨é”šå®šäºŒåˆ†ç±»é˜ˆå€¼ (Ï„={tau}): {binary_threshold:.6f}")
                else:
                    binary_threshold = self.chemical_scores['S_c'].median()

            # ä¸‰åˆ†ç±»é˜ˆå€¼
            if 'ternary_low' in nested_cv_results and 'ternary_high' in nested_cv_results:
                ternary_low = nested_cv_results['ternary_low']['threshold']
                ternary_high = nested_cv_results['ternary_high']['threshold']
                print(f"åµŒå¥—CVä¸‰åˆ†ç±»é˜ˆå€¼: ä½={ternary_low:.6f}, é«˜={ternary_high:.6f}")
            else:
                # å›é€€åˆ°ç®€å•æ–¹æ³•
                print("ä¸‰åˆ†ç±»é˜ˆå€¼å›é€€åˆ°ç®€å•å¤–éƒ¨é”šå®š...")
                tau_low, tau_high = 30, 3

                y_true_low = (self.validation_data['POD_MGKGDAY'] <= tau_low).astype(int)
                if len(np.unique(y_true_low)) > 1:
                    fpr_low, tpr_low, thresholds_low = roc_curve(y_true_low, self.validation_data['S_c'])
                    youden_j_low = tpr_low - fpr_low
                    best_idx_low = np.argmax(youden_j_low)
                    ternary_low = thresholds_low[best_idx_low]
                else:
                    ternary_low = self.chemical_scores['S_c'].quantile(0.33)

                y_true_high = (self.validation_data['POD_MGKGDAY'] <= tau_high).astype(int)
                if len(np.unique(y_true_high)) > 1:
                    fpr_high, tpr_high, thresholds_high = roc_curve(y_true_high, self.validation_data['S_c'])
                    youden_j_high = tpr_high - fpr_high
                    best_idx_high = np.argmax(youden_j_high)
                    ternary_high = thresholds_high[best_idx_high]
                else:
                    ternary_high = self.chemical_scores['S_c'].quantile(0.67)

            # ä¿å­˜åµŒå¥—CVç»“æœ
            self.nested_cv_results = nested_cv_results

        else:
            print("âš ï¸ éªŒè¯æ•°æ®ä¸è¶³(<30)ï¼Œä½¿ç”¨åˆ†ä½æ•°é˜ˆå€¼")
            binary_threshold = self.chemical_scores['S_c'].median()
            ternary_low = self.chemical_scores['S_c'].quantile(0.33)
            ternary_high = self.chemical_scores['S_c'].quantile(0.67)

        # ä¸‰åˆ†ç±»é˜ˆå€¼é—´éš”ä¿æŠ¤ï¼šT_high - T_low â‰¥ max(0.02, 0.5Ã—IQR(S_c))
        iqr = self.chemical_scores['S_c'].quantile(0.75) - self.chemical_scores['S_c'].quantile(0.25)
        min_gap = max(0.02, 0.5 * iqr)

        print(f"ä¸‰åˆ†ç±»é˜ˆå€¼é—´éš”ä¿æŠ¤æ£€æŸ¥:")
        print(f"  IQR(S_c): {iqr:.4f}")
        print(f"  æœ€å°é—´è·è¦æ±‚: {min_gap:.4f}")
        print(f"  å½“å‰é—´è·: {ternary_high - ternary_low:.4f}")

        # æ£€æŸ¥é¡ºåºå’Œé—´è·çº¦æŸ
        valid_order = ternary_low <= binary_threshold <= ternary_high
        sufficient_gap = (ternary_high - ternary_low) >= min_gap

        print(f"çº¦æŸæ£€æŸ¥:")
        print(f"  é¡ºåºæ£€æŸ¥: {valid_order} (è¦æ±‚: T_low â‰¤ T_bin â‰¤ T_high)")
        print(f"  é—´è·æ£€æŸ¥: {sufficient_gap} (è¦æ±‚: T_high - T_low â‰¥ {min_gap:.4f})")

        if not valid_order or not sufficient_gap:
            print("âš ï¸ é˜ˆå€¼ä¸æ»¡è¶³çº¦æŸï¼Œåº”ç”¨å¼ºåˆ¶ä¿æŠ¤æªæ–½")

            # æ­¥éª¤1ï¼šå…ˆä¿è¯é¡ºåºï¼ˆä»¥T_binä¸ºä¸­å¿ƒé‡æ’ï¼‰
            if not valid_order:
                print("  ä¿®æ­£é¡ºåºè¿å...")
                # å¦‚æœé¡ºåºé”™è¯¯ï¼Œé‡æ–°æ’åº
                thresholds = sorted([ternary_low, binary_threshold, ternary_high])
                ternary_low, binary_threshold, ternary_high = thresholds[0], thresholds[1], thresholds[2]

            # æ­¥éª¤2ï¼šå¼ºåˆ¶æ»¡è¶³æœ€å°é—´éš”ï¼ˆä»¥T_binä¸ºä¸­å¿ƒå¯¹ç§°æ‰©å¼ ï¼‰
            current_gap = ternary_high - ternary_low
            if current_gap < min_gap:
                print(f"  å¼ºåˆ¶æ‰©å¼ é—´éš”: {current_gap:.6f} â†’ {min_gap:.6f}")

                # ä»¥T_binä¸ºä¸­å¿ƒå¯¹ç§°æ‰©å¼ 
                delta = min_gap / 2
                ternary_low_new = binary_threshold - delta
                ternary_high_new = binary_threshold + delta

                # è¾¹ç•Œæ£€æŸ¥å’Œè°ƒæ•´
                s_min, s_max = self.chemical_scores['S_c'].min(), self.chemical_scores['S_c'].max()

                if ternary_low_new < s_min:
                    # å·¦è¾¹ç•Œè´´è¾¹ï¼Œæ•´ä½“å³ç§»
                    shift = s_min - ternary_low_new
                    ternary_low_new = s_min
                    ternary_high_new += shift
                elif ternary_high_new > s_max:
                    # å³è¾¹ç•Œè´´è¾¹ï¼Œæ•´ä½“å·¦ç§»
                    shift = ternary_high_new - s_max
                    ternary_high_new = s_max
                    ternary_low_new -= shift

                # æœ€ç»ˆè¾¹ç•Œä¿æŠ¤
                ternary_low_new = max(s_min, ternary_low_new)
                ternary_high_new = min(s_max, ternary_high_new)

                print(f"ä¿æŠ¤å‰: T_low={ternary_low:.6f}, T_bin={binary_threshold:.6f}, T_high={ternary_high:.6f}")
                print(f"ä¿æŠ¤å: T_low={ternary_low_new:.6f}, T_bin={binary_threshold:.6f}, T_high={ternary_high_new:.6f}")

                ternary_low = ternary_low_new
                ternary_high = ternary_high_new

                # æœ€ç»ˆéªŒè¯
                final_gap = ternary_high - ternary_low
                final_order = ternary_low <= binary_threshold <= ternary_high
                print(f"æœ€ç»ˆéªŒè¯: é—´è·={final_gap:.6f} (â‰¥{min_gap:.4f}), é¡ºåº={final_order}")

                if final_gap < min_gap or not final_order:
                    print("âš ï¸ å¼ºåˆ¶ä¿æŠ¤ä»ä¸æ»¡è¶³ï¼Œä½¿ç”¨åˆ†ä½æ•°å›é€€æ–¹æ¡ˆ")
                    ternary_low = self.chemical_scores['S_c'].quantile(0.25)
                    ternary_high = self.chemical_scores['S_c'].quantile(0.75)
        else:
            print("âœ… é˜ˆå€¼æ»¡è¶³æ‰€æœ‰çº¦æŸï¼Œæ— éœ€ä¿æŠ¤")

        print(f"æœ€ç»ˆåˆ†ç±»é˜ˆå€¼:")
        print(f"  äºŒåˆ†ç±»: {binary_threshold:.6f}")
        print(f"  ä¸‰åˆ†ç±»ä½: {ternary_low:.6f}")
        print(f"  ä¸‰åˆ†ç±»é«˜: {ternary_high:.6f}")

        # ä¿å­˜æœ€ç»ˆé˜ˆå€¼ï¼ˆç”¨äºå¯è¿½æº¯æ€§æŠ¥å‘Šï¼‰
        self.final_binary_threshold = binary_threshold
        self.final_ternary_low = ternary_low
        self.final_ternary_high = ternary_high
        self.final_min_gap = min_gap

        # åˆ›å»ºæ ‡ç­¾
        labeled_scores = self.chemical_scores.copy()

        # äºŒåˆ†ç±»ï¼ˆç¨³å¥åˆ†ç±»ï¼‰
        labeled_scores['tcpl_binary_compliant'] = 0
        high_confidence = labeled_scores['ci_lower'] >= binary_threshold
        low_confidence = labeled_scores['ci_upper'] < binary_threshold
        uncertain = ~(high_confidence | low_confidence)

        labeled_scores.loc[high_confidence, 'tcpl_binary_compliant'] = 1
        labeled_scores.loc[low_confidence, 'tcpl_binary_compliant'] = 0
        labeled_scores.loc[uncertain, 'tcpl_binary_compliant'] = (
            labeled_scores.loc[uncertain, 'S_c'] >= binary_threshold
        ).astype(int)

        # ä¸‰åˆ†ç±»
        labeled_scores['tcpl_ternary_compliant'] = 1  # é»˜è®¤ä¸­ç­‰
        labeled_scores.loc[labeled_scores['ci_upper'] < ternary_low, 'tcpl_ternary_compliant'] = 0
        labeled_scores.loc[labeled_scores['ci_lower'] >= ternary_high, 'tcpl_ternary_compliant'] = 2

        # ç»Ÿè®¡åˆ†å¸ƒ
        binary_dist = labeled_scores['tcpl_binary_compliant'].value_counts().sort_index()
        ternary_dist = labeled_scores['tcpl_ternary_compliant'].value_counts().sort_index()

        print(f"æ ‡ç­¾åˆ†å¸ƒ:")
        print(f"  äºŒåˆ†ç±» - ä½æ¯’æ€§: {binary_dist.get(0, 0):,}, é«˜æ¯’æ€§: {binary_dist.get(1, 0):,}")
        print(f"  ä¸‰åˆ†ç±» - ä½æ¯’æ€§: {ternary_dist.get(0, 0):,}, ä¸­ç­‰: {ternary_dist.get(1, 0):,}, é«˜æ¯’æ€§: {ternary_dist.get(2, 0):,}")

        # å»ºç«‹å®Œæ•´æ˜ å°„é“¾ï¼šchid â†’ CAS â†’ PUBCHEM_CID
        print("\nå»ºç«‹å®Œæ•´æ˜ å°„é“¾...")

        # æ­¥éª¤1: chid â†’ CAS (ä»æ¡¥è¡¨)
        chid_cas = self.bridge_table[['chid', 'casn']].dropna()

        # æ­¥éª¤2: CAS â†’ PUBCHEM_CID (ä»ä¸‹è½½è¿›åº¦)
        cas_pubchem_df = pd.DataFrame([
            {'casn': cas, 'PUBCHEM_CID': pubchem_cid}
            for cas, pubchem_cid in self.cas_pubchem_mapping.items()
        ])

        # æ­¥éª¤3: åˆå¹¶æ˜ å°„é“¾
        complete_mapping = chid_cas.merge(
            cas_pubchem_df,
            on='casn',
            how='inner'
        )

        print(f"å®Œæ•´æ˜ å°„é“¾ chidâ†’PUBCHEM_CID: {len(complete_mapping):,} ä¸ª")

        # æ­¥éª¤4: æ·»åŠ æ ‡ç­¾åˆ°æ˜ å°„
        mapping_with_labels = complete_mapping.merge(
            labeled_scores[['chid', 'S_c', 'ci_lower', 'ci_upper',
                           'tcpl_binary_compliant', 'tcpl_ternary_compliant',
                           'n_tested', 'n_positive']],
            on='chid',
            how='inner'
        )

        print(f"æœ‰æ ‡ç­¾çš„æ˜ å°„: {len(mapping_with_labels):,} ä¸ª")

        # æ­¥éª¤5: LEFT JOINåˆ°åŸå§‹processed_final8k213.csv
        final_data = self.original_data.copy()

        # åˆ›å»ºPUBCHEM_CIDåˆ°æ ‡ç­¾çš„æ˜ å°„
        pubchem_to_labels = {}
        for _, row in mapping_with_labels.iterrows():
            pubchem_cid = row['PUBCHEM_CID']
            pubchem_to_labels[pubchem_cid] = {
                'S_c_tcpl_compliant': row['S_c'],
                'S_c_ci_lower_compliant': row['ci_lower'],
                'S_c_ci_upper_compliant': row['ci_upper'],
                'tcpl_binary_compliant': row['tcpl_binary_compliant'],
                'tcpl_ternary_compliant': row['tcpl_ternary_compliant'],
                'tcpl_n_tested_compliant': row['n_tested'],
                'tcpl_n_positive_compliant': row['n_positive']
            }

        # åˆå§‹åŒ–æ–°åˆ—
        for col in pubchem_to_labels[list(pubchem_to_labels.keys())[0]].keys():
            if 'tcpl_binary' in col or 'tcpl_ternary' in col:
                final_data[col] = -1
            elif 'tcpl_n_' in col:
                final_data[col] = 0
            else:
                final_data[col] = -1.0

        # å¡«å……æ ‡ç­¾
        for idx, row in final_data.iterrows():
            pubchem_cid = row['PUBCHEM_CID']
            if pubchem_cid in pubchem_to_labels:
                labels = pubchem_to_labels[pubchem_cid]
                for col, value in labels.items():
                    final_data.loc[idx, col] = value

        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        total_records = len(final_data)
        tcpl_labeled = (final_data['tcpl_binary_compliant'] != -1).sum()

        print(f"\næœ€ç»ˆæ•°æ®é›†:")
        print(f"  æ€»è®°å½•æ•°: {total_records:,}")
        print(f"  tcplæ ‡ç­¾è¦†ç›–: {tcpl_labeled:,} ({tcpl_labeled/total_records*100:.1f}%)")

        if tcpl_labeled > 0:
            final_binary_dist = final_data[final_data['tcpl_binary_compliant'] != -1]['tcpl_binary_compliant'].value_counts()
            final_ternary_dist = final_data[final_data['tcpl_ternary_compliant'] != -1]['tcpl_ternary_compliant'].value_counts()

            print(f"æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ:")
            print(f"  äºŒåˆ†ç±» - ä½æ¯’æ€§: {final_binary_dist.get(0, 0):,}, é«˜æ¯’æ€§: {final_binary_dist.get(1, 0):,}")
            print(f"  ä¸‰åˆ†ç±» - ä½æ¯’æ€§: {final_ternary_dist.get(0, 0):,}, ä¸­ç­‰: {final_ternary_dist.get(1, 0):,}, é«˜æ¯’æ€§: {final_ternary_dist.get(2, 0):,}")

        self.final_labeled_data = final_data
        self.labeled_scores = labeled_scores
        return final_data

    def save_results_to_original_file(self):
        """ä¿å­˜ç»“æœåˆ°åŸå§‹æ–‡ä»¶ï¼ˆæŒ‰éœ€æ±‚ï¼‰"""
        print("\nğŸ’¾ ä¿å­˜ç»“æœåˆ°åŸå§‹æ–‡ä»¶")
        print("-" * 50)

        # ä¿å­˜åˆ°åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆç”Ÿæˆ_labeledç‰ˆæœ¬ï¼‰
        original_file = 'output/data/processed_final8k213.csv'
        labeled_file = 'output/data/processed_final8k213_labeled.csv'

        # åŒæ—¶ä¿å­˜åˆ°GITHUBäº¤ä»˜æ–‡ä»¶å¤¹
        github_labeled_file = 'output/GITHUB/data/processed_final8k213_tcpl_labeled_final.csv'
        os.makedirs('output/GITHUB/data', exist_ok=True)

        # ä¿å­˜æ ‡è®°ç‰ˆæœ¬
        self.final_labeled_data.to_csv(labeled_file, index=False)
        print(f"âœ… æ ‡è®°æ•°æ®é›†: {labeled_file}")

        # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f'output/data/processed_final8k213_tcpl_fully_compliant_{timestamp}.csv'
        self.final_labeled_data.to_csv(backup_file, index=False)
        print(f"âœ… å¤‡ä»½æ–‡ä»¶: {backup_file}")

        # ä¿å­˜åŒ–å­¦å“åˆ†æ•°è¯¦è¡¨
        scores_file = f'output/data/tcpl_fully_compliant_scores_{timestamp}.csv'
        self.labeled_scores.to_csv(scores_file, index=False)
        print(f"âœ… åŒ–å­¦å“åˆ†æ•°: {scores_file}")

        # ä¿å­˜éªŒè¯ç»“æœ
        if hasattr(self, 'validation_data'):
            validation_file = f'output/data/tcpl_fully_compliant_validation_{timestamp}.csv'
            self.validation_data.to_csv(validation_file, index=False)
            print(f"âœ… éªŒè¯ç»“æœ: {validation_file}")

        # ä¿å­˜SHA256å®Œæ•´æ€§æŠ¥å‘Š
        sha256_report_file = f'output/reports/tcpl_data_integrity_sha256_{timestamp}.json'
        os.makedirs('output/reports', exist_ok=True)

        # æ·»åŠ MC5-6æ–‡ä»¶çš„SHA256ï¼ˆå¤§æ–‡ä»¶ï¼Œå•ç‹¬å¤„ç†ï¼‰
        mc56_file = 'data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/mc5-6_winning_model_fits-flags_invitrodbv4_2_SEPT2024.csv'
        if not hasattr(self, 'mc56_hash'):
            print("è®¡ç®—MC5-6æ–‡ä»¶SHA256ï¼ˆå¤§æ–‡ä»¶ï¼Œéœ€è¦æ—¶é—´ï¼‰...")
            self.file_hashes['mc56'] = self.calculate_file_sha256(mc56_file)

        sha256_report = {
            "timestamp": datetime.now().isoformat(),
            "tcpl_compliance_version": "fully_compliant",
            "data_files": {
                "sc2": {
                    "file": "sc1_sc2_invitrodb_v4_2_SEPT2024.xlsx",
                    "sha256": self.file_hashes.get('sc2'),
                    "purpose": "Official hitc values (-1/0/1)",
                    "columns_used": ["chid", "aeid", "hitc", "casn"]
                },
                "mc56": {
                    "file": "mc5-6_winning_model_fits-flags_invitrodbv4_2_SEPT2024.csv",
                    "sha256": self.file_hashes.get('mc56'),
                    "purpose": "mc6_flags and ac50/conc_unit only",
                    "columns_used": ["chid", "aeid", "mc6_flags", "ac50", "conc_unit"]
                },
                "cytotox": {
                    "file": "cytotox_invitrodb_v4_2_SEPT2024.xlsx",
                    "sha256": self.file_hashes.get('cytotox'),
                    "purpose": "Cytotoxicity burst filtering",
                    "columns_used": ["chid", "cytotox_lower_bound_log"]
                },
                "assay_annotations": {
                    "file": "assay_annotations_invitrodb_v4_2_SEPT2024.xlsx",
                    "sha256": self.file_hashes.get('assay_annotations'),
                    "purpose": "Endpoint to mechanism mapping",
                    "columns_used": ["aeid", "intended_target_family"]
                },
                "toxrefdb": {
                    "file": "tox21_toxrefdb_matched_via_cas.csv",
                    "sha256": self.file_hashes.get('toxrefdb'),
                    "purpose": "External validation POD values",
                    "columns_used": ["CAS_NORM", "POD_MGKGDAY", "PUBCHEM_CID"]
                }
            },
            "mapping_files": {
                "cas_mapping": {
                    "file": "cas_download_progress.json",
                    "sha256": self.file_hashes.get('cas_mapping'),
                    "purpose": "CAS to PUBCHEM_CID mapping"
                },
                "bridge_table": {
                    "file": "chemical_bridge_table.csv",
                    "sha256": self.file_hashes.get('bridge_table'),
                    "purpose": "chid to CAS mapping"
                }
            }
        }

        with open(sha256_report_file, 'w') as f:
            json.dump(sha256_report, f, indent=2)

        print(f"âœ… SHA256å®Œæ•´æ€§æŠ¥å‘Š: {sha256_report_file}")

        # ä¿å­˜å¯è¿½æº¯æ€§æŠ¥å‘Š
        if hasattr(self, 'nested_cv_detailed_results'):
            reproducibility_file = f'output/reports/tcpl_reproducibility_report_{timestamp}.json'

            reproducibility_report = {
                "timestamp": datetime.now().isoformat(),
                "random_seeds": {
                    "outer_cv": 42,
                    "inner_cv": 42,
                    "bootstrap_ci": 42,
                    "mechanism_bootstrap": 42
                },
                "nested_cv_configuration": {
                    "outer_folds": 5,
                    "inner_folds": 3,
                    "stratified": True,
                    "threshold_selection_metric": "Youden_J",
                    "candidate_thresholds": "percentiles_5_to_95"
                },
                "final_thresholds": {
                    "binary": getattr(self, 'final_binary_threshold', None),
                    "ternary_low": getattr(self, 'final_ternary_low', None),
                    "ternary_high": getattr(self, 'final_ternary_high', None),
                    "min_gap_enforced": getattr(self, 'final_min_gap', 0.0894),
                    "gap_protection_applied": True
                },
                "ac50_conversion_log": getattr(self, 'ac50_conversion_log', {}),
                "nested_cv_results": getattr(self, 'nested_cv_results', {}),
                "detailed_fold_results": getattr(self, 'nested_cv_detailed_results', {})
            }

            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                return obj

            reproducibility_report_clean = convert_numpy_types(reproducibility_report)

            with open(reproducibility_file, 'w') as f:
                json.dump(reproducibility_report_clean, f, indent=2)

            print(f"âœ… å¯è¿½æº¯æ€§æŠ¥å‘Š: {reproducibility_file}")

        return labeled_file, backup_file, scores_file

    def run_fully_compliant_system(self):
        """è¿è¡Œå®Œå…¨åˆè§„çš„tcplç³»ç»Ÿ"""
        print("ğŸš€ å®Œå…¨åˆè§„çš„tcplç³»ç»Ÿ")
        print("=" * 60)
        print("ä¸¥æ ¼å®ç°æ‰€æœ‰6ä¸ªé—®é¢˜çš„ä¿®æ­£")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        try:
            # 1. åŠ è½½æ‰€æœ‰æ•°æ®æº
            print("\næ­¥éª¤1: åŠ è½½æ‰€æœ‰æ•°æ®æº")
            if not self.load_all_data_sources():
                return None

            # 2. åˆ›å»ºç«¯ç‚¹â†’æœºåˆ¶æ˜ å°„
            print("\næ­¥éª¤2: åˆ›å»ºç«¯ç‚¹â†’æœºåˆ¶æ˜ å°„")
            if not self.create_mechanism_mapping():
                return None

            # 3. å…¨é‡éå†MC5-6æ•°æ®
            print("\næ­¥éª¤3: å…¨é‡éå†MC5-6æ•°æ®")
            if not self.load_mc56_data_full_traversal():
                return None

            # 4. åº”ç”¨tcplåˆè§„è¿‡æ»¤ï¼ˆåŒ…å«æ­£ç¡®ç»†èƒæ¯’æ§åˆ¶ï¼‰
            print("\næ­¥éª¤4: åº”ç”¨tcplåˆè§„è¿‡æ»¤")
            self.apply_tcpl_compliant_filtering_with_cytotox()

            # 5. è®¡ç®—æœºåˆ¶ç­‰æƒBeta-Binomialåˆ†æ•°
            print("\næ­¥éª¤5: è®¡ç®—æœºåˆ¶ç­‰æƒåˆ†æ•°")
            self.calculate_mechanism_weighted_scores()

            # 6. å¤–éƒ¨éªŒè¯ï¼ˆæ­£ç¡®æ˜ å°„é“¾è·¯ï¼‰
            print("\næ­¥éª¤6: å¤–éƒ¨éªŒè¯")
            self.external_validation_correct_mapping()

            # 7. åˆ›å»ºæ ‡ç­¾å¹¶åˆå¹¶åˆ°åŸå§‹æ–‡ä»¶
            print("\næ­¥éª¤7: åˆ›å»ºæ ‡ç­¾å¹¶åˆå¹¶")
            self.create_labels_and_merge_to_original()

            # 8. ä¿å­˜ç»“æœ
            print("\næ­¥éª¤8: ä¿å­˜ç»“æœ")
            labeled_file, backup_file, scores_file = self.save_results_to_original_file()

            # 9. æœ€ç»ˆæ€»ç»“
            self.print_compliance_summary()

            print(f"\nğŸ‰ å®Œå…¨åˆè§„tcplç³»ç»Ÿå®Œæˆ!")
            print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"âœ… ä¸»è¦è¾“å‡º: {labeled_file}")

            return self.final_labeled_data

        except Exception as e:
            print(f"\nâŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def print_compliance_summary(self):
        """æ‰“å°åˆè§„æ€§æ€»ç»“"""
        print("\nğŸ“‹ åˆè§„æ€§æ£€æŸ¥æ€»ç»“")
        print("=" * 60)

        print("âœ… é—®é¢˜1: å‘½ä¸­æ•°æ®æºé€‰å–ç¨³å®š")
        print("   - ä½¿ç”¨SC2çš„hitcâˆˆ{-1,0,1}ä¸ºå‡†")
        print("   - MC5-6ä»…ç”¨äºè¡¥å……mc6_flagsã€ac50")
        print("   - é€šè¿‡['chid','aeid']å·¦è¿æ¥")

        print("\nâœ… é—®é¢˜2: ç»†èƒæ¯’è¿‡æ»¤å­—æ®µæ­£ç¡®")
        print("   - ä½¿ç”¨cytotox_lower_bound_logå­—æ®µ")
        print("   - æŒ‰Î”=3çš„log10è·ç¦»åšburstç­›é€‰")

        print("\nâœ… é—®é¢˜3: MC5-6å…¨é‡è¯»å–")
        print("   - å…¨é‡éå†ï¼Œæ— æ—©åœ")
        print("   - æ¯å—å†…å…ˆæŒ‰æ˜ å°„chidè¿‡æ»¤å†ç´¯ç§¯")

        print("\nâœ… é—®é¢˜4: Beta-Binomialæœºåˆ¶ç­‰æƒ")
        print("   - ç«¯ç‚¹â†’æœºåˆ¶(NR/SR/DDR/CYTO)æ˜ å°„")
        print("   - æœºåˆ¶å†…åšæ”¶ç¼©ï¼Œæœºåˆ¶é—´ç­‰æƒå¹³å‡")

        print("\nâœ… é—®é¢˜5: å¤–éƒ¨éªŒè¯æ˜ å°„é“¾è·¯æ­£ç¡®")
        print("   - SC2çš„casnåšåº•åº§")
        print("   - chidâ†’casnâ†’PUBCHEM_CIDå®Œæ•´é“¾è·¯")
        print("   - ä¸ToxRefDBå¯¹é½éªŒè¯")

        print("\nâœ… é—®é¢˜6: æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ­£ç¡®")
        print("   - å¯¹processed_final8k213.csvåšLEFT JOIN")
        print("   - é€šè¿‡PUBCHEM_CIDè¿½åŠ æ ‡ç­¾åˆ—")
        print("   - ç”Ÿæˆ_labeled.csvåŒç›®å½•æ–‡ä»¶")

        if hasattr(self, 'final_labeled_data'):
            total_records = len(self.final_labeled_data)
            tcpl_labeled = (self.final_labeled_data['tcpl_binary_compliant'] != -1).sum()
            print(f"\nğŸ¯ æœ€ç»ˆæˆæœ:")
            print(f"   æ€»è®°å½•æ•°: {total_records:,}")
            print(f"   tcplæ ‡ç­¾è¦†ç›–: {tcpl_labeled:,} ({tcpl_labeled/total_records*100:.1f}%)")
            print(f"   å®Œå…¨åˆè§„å®ç°: âœ…")

def main():
    """ä¸»å‡½æ•°"""
    system = TcplSystemFullyCompliant()
    result = system.run_fully_compliant_system()
    return result

if __name__ == "__main__":
    result = main()
