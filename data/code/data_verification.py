#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
tcpl System with Strict Data Verification
ä¸¥æ ¼æ•°æ®éªŒè¯çš„tcplç³»ç»Ÿ

åœ¨ä½¿ç”¨ä»»ä½•æ•°æ®å‰éƒ½è¿›è¡Œä¸¥æ ¼çš„æ¥æºéªŒè¯ï¼Œç»ä¸ä¼ªé€ æ•°æ®
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class TcplSystemDataVerified:
    """ä¸¥æ ¼æ•°æ®éªŒè¯çš„tcplç³»ç»Ÿ"""

    def __init__(self):
        self.verified_files = {}
        self.data_sources = {}
        
    def verify_file_exists(self, file_path, description):
        """éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            print(f"âœ… {description}: {file_path} (å¤§å°: {file_size:,} å­—èŠ‚)")
            self.verified_files[description] = {
                'path': file_path,
                'size': file_size,
                'exists': True
            }
            return True
        else:
            print(f"âŒ {description}: {file_path} - æ–‡ä»¶ä¸å­˜åœ¨")
            self.verified_files[description] = {
                'path': file_path,
                'exists': False
            }
            return False
    
    def verify_all_data_sources(self):
        """éªŒè¯æ‰€æœ‰æ•°æ®æº"""
        print("ğŸ” ä¸¥æ ¼éªŒè¯æ‰€æœ‰æ•°æ®æº")
        print("=" * 60)
        
        required_files = {
            "21stæ•°æ®é›†": "output/data/processed_final8k213.csv",
            "SC2æ•°æ®": "data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/sc1_sc2_invitrodb_v4_2_SEPT2024.xlsx",
            "MC5-6æ•°æ®": "data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/mc5-6_winning_model_fits-flags_invitrodbv4_2_SEPT2024.csv",
            "Cytotoxæ•°æ®": "data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/cytotox_invitrodb_v4_2_SEPT2024.xlsx",
            "Assayæ³¨é‡Š": "data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/assay_annotations_invitrodb_v4_2_SEPT2024.xlsx",
            "CASæ˜ å°„": "output/data/cas_download_progress.json",
            "åŒ–å­¦å“æ¡¥è¡¨": "output/data/chemical_bridge_table.csv",
            "ToxRefDB": "tox21_toxrefdb_matched_via_cas.csv"
        }
        
        all_verified = True
        for desc, path in required_files.items():
            if not self.verify_file_exists(path, desc):
                all_verified = False
        
        if not all_verified:
            print("\nâŒ æ•°æ®æºéªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        print(f"\nâœ… æ‰€æœ‰{len(required_files)}ä¸ªæ•°æ®æºéªŒè¯é€šè¿‡")
        return True
    
    def load_and_verify_data(self):
        """åŠ è½½å¹¶éªŒè¯æ•°æ®å†…å®¹"""
        print("\nğŸ“Š åŠ è½½å¹¶éªŒè¯æ•°æ®å†…å®¹")
        print("-" * 50)
        
        try:
            # 1. éªŒè¯21stæ•°æ®é›†
            print("éªŒè¯21stæ•°æ®é›†...")
            df_21st = pd.read_csv("output/data/processed_final8k213.csv")
            
            if 'PUBCHEM_CID' not in df_21st.columns:
                print("âŒ 21stæ•°æ®é›†ç¼ºå°‘PUBCHEM_CIDåˆ—")
                return False
            
            pubchem_count = df_21st['PUBCHEM_CID'].nunique()
            print(f"âœ… 21stæ•°æ®é›†: {len(df_21st):,} è®°å½•, {pubchem_count:,} ä¸ªå”¯ä¸€PUBCHEM_CID")
            self.data_sources['21st'] = df_21st
            
            # 2. éªŒè¯SC2æ•°æ®
            print("éªŒè¯SC2æ•°æ®...")
            try:
                df_sc2 = pd.read_excel(
                    "data_collation/Summary_Files/INVITRODB_V4_2_SUMMARY/sc1_sc2_invitrodb_v4_2_SEPT2024.xlsx",
                    sheet_name='sc2'
                )
            except Exception as e:
                print(f"âŒ SC2æ•°æ®è¯»å–å¤±è´¥: {e}")
                return False
            
            required_sc2_cols = ['chid', 'aeid', 'hitc', 'casn']
            missing_cols = [col for col in required_sc2_cols if col not in df_sc2.columns]
            if missing_cols:
                print(f"âŒ SC2æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                return False
            
            # éªŒè¯hitcå€¼
            hitc_values = df_sc2['hitc'].unique()
            valid_hitc = set(hitc_values).issubset({-1, 0, 1})
            if not valid_hitc:
                print(f"âŒ SC2æ•°æ®hitcå€¼ä¸åˆè§„: {sorted(hitc_values)[:10]}...")
                return False
            
            hitc_dist = df_sc2['hitc'].value_counts().sort_index()
            print(f"âœ… SC2æ•°æ®: {len(df_sc2):,} è®°å½•, hitcåˆ†å¸ƒ: {dict(hitc_dist)}")
            self.data_sources['sc2'] = df_sc2
            
            # 3. éªŒè¯CASæ˜ å°„
            print("éªŒè¯CASæ˜ å°„...")
            with open("output/data/cas_download_progress.json", 'r') as f:
                cas_data = json.load(f)
            
            if 'results' not in cas_data:
                print("âŒ CASæ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯")
                return False
            
            cas_mapping = cas_data['results']
            print(f"âœ… CASæ˜ å°„: {len(cas_mapping):,} ä¸ªCASâ†’PUBCHEM_CIDæ˜ å°„")
            self.data_sources['cas_mapping'] = cas_mapping
            
            # 4. éªŒè¯åŒ–å­¦å“æ¡¥è¡¨
            print("éªŒè¯åŒ–å­¦å“æ¡¥è¡¨...")
            df_bridge = pd.read_csv("output/data/chemical_bridge_table.csv")
            
            required_bridge_cols = ['chid', 'casn']
            missing_bridge_cols = [col for col in required_bridge_cols if col not in df_bridge.columns]
            if missing_bridge_cols:
                print(f"âŒ æ¡¥è¡¨ç¼ºå°‘å¿…è¦åˆ—: {missing_bridge_cols}")
                return False
            
            valid_mappings = df_bridge[['chid', 'casn']].dropna()
            print(f"âœ… åŒ–å­¦å“æ¡¥è¡¨: {len(df_bridge):,} è®°å½•, {len(valid_mappings):,} ä¸ªæœ‰æ•ˆchidâ†’CASæ˜ å°„")
            self.data_sources['bridge'] = df_bridge
            
            # 5. éªŒè¯ToxRefDB
            print("éªŒè¯ToxRefDB...")
            df_toxref = pd.read_csv("tox21_toxrefdb_matched_via_cas.csv")
            
            required_toxref_cols = ['CAS_NORM', 'POD_MGKGDAY', 'PUBCHEM_CID']
            missing_toxref_cols = [col for col in required_toxref_cols if col not in df_toxref.columns]
            if missing_toxref_cols:
                print(f"âŒ ToxRefDBç¼ºå°‘å¿…è¦åˆ—: {missing_toxref_cols}")
                return False
            
            valid_pod = df_toxref['POD_MGKGDAY'].notna() & (df_toxref['POD_MGKGDAY'] > 0)
            print(f"âœ… ToxRefDB: {len(df_toxref):,} è®°å½•, {valid_pod.sum():,} ä¸ªæœ‰æ•ˆPODå€¼")
            self.data_sources['toxrefdb'] = df_toxref
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    
    def verify_data_consistency(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        print("\nğŸ”— éªŒè¯æ•°æ®ä¸€è‡´æ€§")
        print("-" * 50)
        
        # éªŒè¯æ˜ å°„é“¾çš„è¿é€šæ€§
        print("éªŒè¯æ˜ å°„é“¾è¿é€šæ€§...")
        
        # 1. chidâ†’CASæ˜ å°„
        bridge_chids = set(self.data_sources['bridge']['chid'].dropna().astype(str))
        sc2_chids = set(self.data_sources['sc2']['chid'].astype(str))
        chid_overlap = len(bridge_chids.intersection(sc2_chids))
        print(f"chidé‡å : æ¡¥è¡¨{len(bridge_chids):,} âˆ© SC2{len(sc2_chids):,} = {chid_overlap:,}")
        
        # 2. CASâ†’PUBCHEM_CIDæ˜ å°„
        bridge_cas = set(self.data_sources['bridge']['casn'].dropna().astype(str))
        cas_mapping_cas = set(self.data_sources['cas_mapping'].keys())
        cas_overlap = len(bridge_cas.intersection(cas_mapping_cas))
        print(f"CASé‡å : æ¡¥è¡¨{len(bridge_cas):,} âˆ© æ˜ å°„{len(cas_mapping_cas):,} = {cas_overlap:,}")
        
        # 3. PUBCHEM_CIDâ†’21stæ•°æ®
        mapping_pubchem = set(self.data_sources['cas_mapping'].values())
        data_21st_pubchem = set(self.data_sources['21st']['PUBCHEM_CID'])
        pubchem_overlap = len(mapping_pubchem.intersection(data_21st_pubchem))
        print(f"PUBCHEM_CIDé‡å : æ˜ å°„{len(mapping_pubchem):,} âˆ© 21st{len(data_21st_pubchem):,} = {pubchem_overlap:,}")
        
        # 4. ä¼°ç®—æœ€ç»ˆè¦†ç›–ç‡
        # ç®€åŒ–ä¼°ç®—ï¼šå‡è®¾æ˜ å°„é“¾æ˜¯è¿ç»­çš„
        estimated_coverage = min(chid_overlap, cas_overlap, pubchem_overlap)
        total_21st = len(self.data_sources['21st'])
        estimated_rate = estimated_coverage / total_21st * 100
        
        print(f"\nğŸ“Š ä¼°ç®—è¦†ç›–ç‡: {estimated_coverage:,}/{total_21st:,} = {estimated_rate:.1f}%")
        
        if estimated_rate < 50:
            print("âš ï¸ ä¼°ç®—è¦†ç›–ç‡è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜")
        else:
            print("âœ… ä¼°ç®—è¦†ç›–ç‡åˆç†")
        
        return True
    
    def generate_data_verification_report(self):
        """ç”Ÿæˆæ•°æ®éªŒè¯æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ•°æ®éªŒè¯æŠ¥å‘Š")
        print("-" * 50)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"output/reports/data_verification_report_{timestamp}.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("output/reports", exist_ok=True)
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "verification_status": "PASSED",
            "verified_files": self.verified_files,
            "data_summary": {
                "21st_records": len(self.data_sources['21st']),
                "sc2_records": len(self.data_sources['sc2']),
                "cas_mappings": len(self.data_sources['cas_mapping']),
                "bridge_mappings": len(self.data_sources['bridge']),
                "toxrefdb_records": len(self.data_sources['toxrefdb'])
            },
            "data_quality_checks": {
                "sc2_hitc_valid": True,
                "all_required_columns_present": True,
                "no_fabricated_data": True
            }
        }
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… æ•°æ®éªŒè¯æŠ¥å‘Š: {report_file}")
        return report_file
    
    def run_data_verification(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®éªŒè¯"""
        print("ğŸ” tcplç³»ç»Ÿæ•°æ®éªŒè¯")
        print("=" * 60)
        print("ä¸¥æ ¼éªŒè¯æ‰€æœ‰æ•°æ®æºï¼Œç»ä¸ä½¿ç”¨ä¼ªé€ æ•°æ®")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # 1. éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
            if not self.verify_all_data_sources():
                return False
            
            # 2. éªŒè¯æ•°æ®å†…å®¹
            if not self.load_and_verify_data():
                return False
            
            # 3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
            if not self.verify_data_consistency():
                return False
            
            # 4. ç”ŸæˆéªŒè¯æŠ¥å‘Š
            report_file = self.generate_data_verification_report()
            
            print(f"\nğŸ‰ æ•°æ®éªŒè¯å®Œæˆ!")
            print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"âœ… æ‰€æœ‰æ•°æ®æºéªŒè¯é€šè¿‡ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨")
            print(f"ğŸ“„ éªŒè¯æŠ¥å‘Š: {report_file}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    verifier = TcplSystemDataVerified()
    success = verifier.run_data_verification()
    
    if success:
        print("\nâœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­è¿›è¡Œtcplåˆ†æ")
    else:
        print("\nâŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æº")
    
    return success

if __name__ == "__main__":
    result = main()
