2025-09-07 16:17:48,761 - INFO - üî¨ Starting experiment: r1c3_single_cls_0_seed_42
2025-09-07 16:17:48,761 - INFO - üìÅ Experiment directory: experiments/r1c3_single_cls_0_seed_42_20250907_161748
2025-09-07 16:17:48,761 - INFO - üé≤ Random seed: 42
2025-09-07 16:17:48,761 - INFO - üîí Deterministic mode: True
2025-09-07 16:17:48,761 - INFO - Using device: cuda
2025-09-07 16:17:48,761 - INFO - Total GPU Memory: 25.16 GB
2025-09-07 16:17:48,781 - INFO - üß™ Fusion: Using cross-attention dynamic fusion
2025-09-07 16:17:48,781 - INFO - üß™ GNN Backbone: Using default GraphAttentionNetwork branch
2025-09-07 16:17:48,781 - INFO - üéØ Task Mode: Single classification endpoint 0
2025-09-07 16:17:48,782 - INFO - Configuration saved to: experiments/r1c3_single_cls_0_seed_42_20250907_161748/checkpoints/config.json
2025-09-07 16:17:48,782 - INFO - üöÄ Using preprocessed data from: data/data/processed
2025-09-07 16:17:48,782 - INFO - Loading LMDB dataset from: data/data/processed
2025-09-07 16:17:49,318 - INFO - Number of training batches: 1139
2025-09-07 16:17:49,318 - INFO - Number of validation batches: 1139
2025-09-07 16:17:49,318 - INFO - Number of test batches: 1139
2025-09-07 16:17:49,318 - INFO - Creating ToxD4C model...
2025-09-07 16:17:50,524 - INFO - Model created successfully. Total parameters: 100,932,036
2025-09-07 16:17:50,525 - INFO - Testing model forward pass...
2025-09-07 16:17:51,055 - INFO - Test output shapes:
2025-09-07 16:17:51,055 - INFO -   classification: torch.Size([32, 1])
2025-09-07 16:17:51,062 - INFO -   classification contains NaN: False
2025-09-07 16:17:51,062 - INFO -   classification contains Inf: False
2025-09-07 16:17:51,062 - INFO -   regression: torch.Size([32, 0])
2025-09-07 16:17:51,066 - INFO -   regression contains NaN: False
2025-09-07 16:17:51,066 - INFO -   regression contains Inf: False
2025-09-07 16:17:51,067 - INFO - Model forward pass test successful!
2025-09-07 16:17:51,068 - INFO - Starting training...
2025-09-07 16:17:51,068 - INFO - 
Epoch 1/15
2025-09-07 16:17:51,398 - INFO - Batch 0/1139: Loss=1.4389, Cls=0.0000, Reg=0.0000, Con=4.7963
2025-09-07 16:17:54,120 - INFO - Batch 10/1139: Loss=2.1677, Cls=0.6819, Reg=0.0000, Con=4.9527
2025-09-07 16:17:56,951 - INFO - Batch 20/1139: Loss=1.5085, Cls=0.0000, Reg=0.0000, Con=5.0283
2025-09-07 16:17:59,744 - INFO - Batch 30/1139: Loss=1.4751, Cls=0.0000, Reg=0.0000, Con=4.9170
2025-09-07 16:18:02,617 - INFO - Batch 40/1139: Loss=2.1402, Cls=0.6993, Reg=0.0000, Con=4.8030
2025-09-07 16:18:05,257 - INFO - Batch 50/1139: Loss=2.1201, Cls=0.6873, Reg=0.0000, Con=4.7759
2025-09-07 16:18:08,092 - INFO - Batch 60/1139: Loss=2.0858, Cls=0.7461, Reg=0.0000, Con=4.4655
2025-09-07 16:18:10,549 - INFO - Batch 70/1139: Loss=2.2217, Cls=0.6876, Reg=0.0000, Con=5.1137
2025-09-07 16:18:13,261 - INFO - Batch 80/1139: Loss=2.1745, Cls=0.7252, Reg=0.0000, Con=4.8310
2025-09-07 16:18:15,870 - INFO - Batch 90/1139: Loss=2.2229, Cls=0.6810, Reg=0.0000, Con=5.1397
2025-09-07 16:18:18,544 - INFO - Batch 100/1139: Loss=1.5878, Cls=0.0000, Reg=0.0000, Con=5.2928
2025-09-07 16:18:21,168 - INFO - Batch 110/1139: Loss=1.4549, Cls=0.0000, Reg=0.0000, Con=4.8496
2025-09-07 16:18:23,699 - INFO - Batch 120/1139: Loss=1.5918, Cls=0.0000, Reg=0.0000, Con=5.3062
2025-09-07 16:18:26,218 - INFO - Batch 130/1139: Loss=1.3909, Cls=0.0000, Reg=0.0000, Con=4.6363
2025-09-07 16:18:28,807 - INFO - Batch 140/1139: Loss=2.2836, Cls=0.7140, Reg=0.0000, Con=5.2319
2025-09-07 16:18:31,435 - INFO - Batch 150/1139: Loss=2.1049, Cls=0.6382, Reg=0.0000, Con=4.8888
2025-09-07 16:18:33,944 - INFO - Batch 160/1139: Loss=1.5500, Cls=0.0000, Reg=0.0000, Con=5.1666
2025-09-07 16:18:36,856 - INFO - Batch 170/1139: Loss=2.1021, Cls=0.6807, Reg=0.0000, Con=4.7382
2025-09-07 16:18:39,591 - INFO - Batch 180/1139: Loss=1.7046, Cls=0.0000, Reg=0.0000, Con=5.6819
2025-09-07 16:18:42,338 - INFO - Batch 190/1139: Loss=2.1279, Cls=0.7070, Reg=0.0000, Con=4.7364
2025-09-07 16:18:45,197 - INFO - Batch 200/1139: Loss=2.0757, Cls=0.6704, Reg=0.0000, Con=4.6841
2025-09-07 16:18:47,877 - INFO - Batch 210/1139: Loss=1.3993, Cls=0.0000, Reg=0.0000, Con=4.6642
2025-09-07 16:18:50,575 - INFO - Batch 220/1139: Loss=2.1840, Cls=0.6910, Reg=0.0000, Con=4.9767
2025-09-07 16:18:53,381 - INFO - Batch 230/1139: Loss=2.2110, Cls=0.7387, Reg=0.0000, Con=4.9078
2025-09-07 16:18:56,228 - INFO - Batch 240/1139: Loss=2.0186, Cls=0.5965, Reg=0.0000, Con=4.7406
2025-09-07 16:18:59,190 - INFO - Batch 250/1139: Loss=2.0008, Cls=0.5915, Reg=0.0000, Con=4.6978
2025-09-07 16:19:01,959 - INFO - Batch 260/1139: Loss=2.0745, Cls=0.6230, Reg=0.0000, Con=4.8384
2025-09-07 16:19:04,545 - INFO - Batch 270/1139: Loss=1.4728, Cls=0.0000, Reg=0.0000, Con=4.9094
2025-09-07 16:19:07,082 - INFO - Batch 280/1139: Loss=1.4282, Cls=0.0000, Reg=0.0000, Con=4.7606
2025-09-07 16:19:09,858 - INFO - Batch 290/1139: Loss=1.4017, Cls=0.0000, Reg=0.0000, Con=4.6724
2025-09-07 16:19:12,719 - INFO - Batch 300/1139: Loss=2.1584, Cls=0.7027, Reg=0.0000, Con=4.8525
2025-09-07 16:19:15,346 - INFO - Batch 310/1139: Loss=1.5195, Cls=0.0000, Reg=0.0000, Con=5.0651
2025-09-07 16:19:18,030 - INFO - Batch 320/1139: Loss=2.3179, Cls=0.7895, Reg=0.0000, Con=5.0948
2025-09-07 16:19:20,572 - INFO - Batch 330/1139: Loss=2.0524, Cls=0.5624, Reg=0.0000, Con=4.9668
2025-09-07 16:19:22,997 - INFO - Batch 340/1139: Loss=2.4193, Cls=0.7857, Reg=0.0000, Con=5.4451
2025-09-07 16:19:25,663 - INFO - Batch 350/1139: Loss=1.3441, Cls=0.0000, Reg=0.0000, Con=4.4804
2025-09-07 16:19:28,412 - INFO - Batch 360/1139: Loss=2.1060, Cls=0.7387, Reg=0.0000, Con=4.5577
2025-09-07 16:19:30,961 - INFO - Batch 370/1139: Loss=1.4859, Cls=0.0000, Reg=0.0000, Con=4.9532
2025-09-07 16:19:33,598 - INFO - Batch 380/1139: Loss=2.1046, Cls=0.6528, Reg=0.0000, Con=4.8394
2025-09-07 16:19:36,421 - INFO - Batch 390/1139: Loss=2.1802, Cls=0.6917, Reg=0.0000, Con=4.9617
2025-09-07 16:19:39,195 - INFO - Batch 400/1139: Loss=2.2107, Cls=0.7273, Reg=0.0000, Con=4.9445
2025-09-07 16:19:41,932 - INFO - Batch 410/1139: Loss=1.4784, Cls=0.0000, Reg=0.0000, Con=4.9280
2025-09-07 16:19:44,793 - INFO - Batch 420/1139: Loss=2.1983, Cls=0.7219, Reg=0.0000, Con=4.9216
2025-09-07 16:19:47,435 - INFO - Batch 430/1139: Loss=1.8069, Cls=0.3703, Reg=0.0000, Con=4.7886
2025-09-07 16:19:50,198 - INFO - Batch 440/1139: Loss=1.5301, Cls=0.0000, Reg=0.0000, Con=5.1004
2025-09-07 16:19:53,074 - INFO - Batch 450/1139: Loss=1.8635, Cls=0.3767, Reg=0.0000, Con=4.9558
2025-09-07 16:19:56,002 - INFO - Batch 460/1139: Loss=2.7127, Cls=1.2265, Reg=0.0000, Con=4.9540
2025-09-07 16:19:58,697 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:20:01,623 - INFO - Batch 480/1139: Loss=1.7099, Cls=0.2245, Reg=0.0000, Con=4.9516
2025-09-07 16:20:04,448 - INFO - Batch 490/1139: Loss=1.4842, Cls=0.0000, Reg=0.0000, Con=4.9472
2025-09-07 16:20:07,264 - INFO - Batch 500/1139: Loss=1.4862, Cls=0.0000, Reg=0.0000, Con=4.9540
2025-09-07 16:20:09,900 - INFO - Batch 510/1139: Loss=1.4865, Cls=0.0000, Reg=0.0000, Con=4.9550
2025-09-07 16:20:12,757 - INFO - Batch 520/1139: Loss=1.8977, Cls=0.4120, Reg=0.0000, Con=4.9521
2025-09-07 16:20:15,577 - INFO - Batch 530/1139: Loss=1.4850, Cls=0.0000, Reg=0.0000, Con=4.9500
2025-09-07 16:20:18,377 - INFO - Batch 540/1139: Loss=2.2634, Cls=0.7786, Reg=0.0000, Con=4.9492
2025-09-07 16:20:21,009 - INFO - Batch 550/1139: Loss=2.0772, Cls=0.5916, Reg=0.0000, Con=4.9520
2025-09-07 16:20:23,787 - INFO - Batch 560/1139: Loss=1.4841, Cls=0.0000, Reg=0.0000, Con=4.9470
2025-09-07 16:20:26,584 - INFO - Batch 570/1139: Loss=2.2591, Cls=0.7742, Reg=0.0000, Con=4.9495
2025-09-07 16:20:29,423 - INFO - Batch 580/1139: Loss=1.9402, Cls=0.4552, Reg=0.0000, Con=4.9501
2025-09-07 16:20:32,249 - INFO - Batch 590/1139: Loss=2.3141, Cls=0.8313, Reg=0.0000, Con=4.9427
2025-09-07 16:20:34,830 - INFO - Batch 600/1139: Loss=1.4788, Cls=0.0000, Reg=0.0000, Con=4.9293
2025-09-07 16:20:37,290 - INFO - Batch 610/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:20:39,945 - INFO - Batch 620/1139: Loss=1.5086, Cls=0.0000, Reg=0.0000, Con=5.0288
2025-09-07 16:20:42,772 - INFO - Batch 630/1139: Loss=2.2851, Cls=0.8819, Reg=0.0000, Con=4.6773
2025-09-07 16:20:45,460 - INFO - Batch 640/1139: Loss=2.2031, Cls=0.7072, Reg=0.0000, Con=4.9865
2025-09-07 16:20:48,244 - INFO - Batch 650/1139: Loss=2.2248, Cls=0.7396, Reg=0.0000, Con=4.9505
2025-09-07 16:20:51,007 - INFO - Batch 660/1139: Loss=1.4861, Cls=0.0000, Reg=0.0000, Con=4.9537
2025-09-07 16:20:53,717 - INFO - Batch 670/1139: Loss=2.0952, Cls=0.6093, Reg=0.0000, Con=4.9531
2025-09-07 16:20:56,456 - INFO - Batch 680/1139: Loss=1.9297, Cls=0.4435, Reg=0.0000, Con=4.9537
2025-09-07 16:20:59,296 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:21:02,196 - INFO - Batch 700/1139: Loss=2.4330, Cls=0.9471, Reg=0.0000, Con=4.9532
2025-09-07 16:21:04,986 - INFO - Batch 710/1139: Loss=2.4987, Cls=1.0132, Reg=0.0000, Con=4.9519
2025-09-07 16:21:07,754 - INFO - Batch 720/1139: Loss=2.2182, Cls=0.7328, Reg=0.0000, Con=4.9513
2025-09-07 16:21:10,628 - INFO - Batch 730/1139: Loss=2.1556, Cls=0.6699, Reg=0.0000, Con=4.9524
2025-09-07 16:21:13,462 - INFO - Batch 740/1139: Loss=1.4873, Cls=0.0000, Reg=0.0000, Con=4.9578
2025-09-07 16:21:16,262 - INFO - Batch 750/1139: Loss=1.4850, Cls=0.0000, Reg=0.0000, Con=4.9499
2025-09-07 16:21:19,164 - INFO - Batch 760/1139: Loss=2.0795, Cls=0.5944, Reg=0.0000, Con=4.9504
2025-09-07 16:21:22,073 - INFO - Batch 770/1139: Loss=2.1390, Cls=0.6538, Reg=0.0000, Con=4.9506
2025-09-07 16:21:24,969 - INFO - Batch 780/1139: Loss=2.2296, Cls=0.7440, Reg=0.0000, Con=4.9518
2025-09-07 16:21:27,720 - INFO - Batch 790/1139: Loss=2.1042, Cls=0.6253, Reg=0.0000, Con=4.9298
2025-09-07 16:21:30,558 - INFO - Batch 800/1139: Loss=2.7536, Cls=1.2385, Reg=0.0000, Con=5.0503
2025-09-07 16:21:33,268 - INFO - Batch 810/1139: Loss=1.4649, Cls=0.0000, Reg=0.0000, Con=4.8829
2025-09-07 16:21:35,903 - INFO - Batch 820/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9522
2025-09-07 16:21:38,833 - INFO - Batch 830/1139: Loss=1.5807, Cls=0.0952, Reg=0.0000, Con=4.9515
2025-09-07 16:21:41,574 - INFO - Batch 840/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9525
2025-09-07 16:21:44,298 - INFO - Batch 850/1139: Loss=1.8352, Cls=0.3487, Reg=0.0000, Con=4.9553
2025-09-07 16:21:47,036 - INFO - Batch 860/1139: Loss=1.4875, Cls=0.0000, Reg=0.0000, Con=4.9584
2025-09-07 16:21:49,789 - INFO - Batch 870/1139: Loss=1.6248, Cls=0.1394, Reg=0.0000, Con=4.9514
2025-09-07 16:21:52,724 - INFO - Batch 880/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9522
2025-09-07 16:21:55,538 - INFO - Batch 890/1139: Loss=1.5916, Cls=0.1065, Reg=0.0000, Con=4.9506
2025-09-07 16:21:58,173 - INFO - Batch 900/1139: Loss=4.0188, Cls=2.5328, Reg=0.0000, Con=4.9532
2025-09-07 16:22:00,769 - INFO - Batch 910/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9523
2025-09-07 16:22:03,575 - INFO - Batch 920/1139: Loss=1.9267, Cls=0.4415, Reg=0.0000, Con=4.9506
2025-09-07 16:22:06,376 - INFO - Batch 930/1139: Loss=1.6923, Cls=0.2063, Reg=0.0000, Con=4.9531
2025-09-07 16:22:09,071 - INFO - Batch 940/1139: Loss=0.3056, Cls=0.3056, Reg=0.0000, Con=0.0000
2025-09-07 16:22:11,877 - INFO - Batch 950/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9522
2025-09-07 16:22:14,793 - INFO - Batch 960/1139: Loss=1.4851, Cls=0.0000, Reg=0.0000, Con=4.9502
2025-09-07 16:22:17,630 - INFO - Batch 970/1139: Loss=2.0336, Cls=0.5473, Reg=0.0000, Con=4.9544
2025-09-07 16:22:20,552 - INFO - Batch 980/1139: Loss=1.9301, Cls=0.4443, Reg=0.0000, Con=4.9529
2025-09-07 16:22:23,460 - INFO - Batch 990/1139: Loss=4.6529, Cls=3.1673, Reg=0.0000, Con=4.9519
2025-09-07 16:22:26,182 - INFO - Batch 1000/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:22:28,894 - INFO - Batch 1010/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:22:31,772 - INFO - Batch 1020/1139: Loss=1.5353, Cls=0.0499, Reg=0.0000, Con=4.9514
2025-09-07 16:22:34,588 - INFO - Batch 1030/1139: Loss=1.4950, Cls=0.0097, Reg=0.0000, Con=4.9511
2025-09-07 16:22:37,414 - INFO - Batch 1040/1139: Loss=2.6055, Cls=1.1199, Reg=0.0000, Con=4.9522
2025-09-07 16:22:40,239 - INFO - Batch 1050/1139: Loss=2.4628, Cls=0.9769, Reg=0.0000, Con=4.9531
2025-09-07 16:22:42,872 - INFO - Batch 1060/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:22:45,692 - INFO - Batch 1070/1139: Loss=1.8058, Cls=0.3204, Reg=0.0000, Con=4.9512
2025-09-07 16:22:48,291 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:22:51,099 - INFO - Batch 1090/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9506
2025-09-07 16:22:53,911 - INFO - Batch 1100/1139: Loss=4.2010, Cls=2.7156, Reg=0.0000, Con=4.9515
2025-09-07 16:22:56,664 - INFO - Batch 1110/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:22:59,223 - INFO - Batch 1120/1139: Loss=4.4899, Cls=3.0047, Reg=0.0000, Con=4.9508
2025-09-07 16:23:01,912 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:26:24,056 - INFO - === Epoch 1 Results ===
2025-09-07 16:26:24,056 - INFO - Training Loss: 1.7908 (Classification: 0.3859, Regression: 0.0000)
2025-09-07 16:26:24,056 - INFO - Validation Loss: 0.1014 (Classification: 0.1014, Regression: 0.0000)
2025-09-07 16:26:24,056 - INFO - Average Classification Accuracy: 0.5521 (on 1/26 tasks)
2025-09-07 16:26:24,056 - INFO - Average AUC: 0.5954
2025-09-07 16:26:25,610 - INFO - Best model saved to: experiments/r1c3_single_cls_0_seed_42_20250907_161748/checkpoints/r1c3_single_cls_0_seed_42_best.pth
2025-09-07 16:26:25,610 - INFO - 
Epoch 2/15
2025-09-07 16:26:25,861 - INFO - Batch 0/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9508
2025-09-07 16:26:28,597 - INFO - Batch 10/1139: Loss=2.7845, Cls=1.2988, Reg=0.0000, Con=4.9522
2025-09-07 16:26:31,287 - INFO - Batch 20/1139: Loss=4.8092, Cls=3.3238, Reg=0.0000, Con=4.9514
2025-09-07 16:26:34,093 - INFO - Batch 30/1139: Loss=1.5090, Cls=0.0220, Reg=0.0000, Con=4.9566
2025-09-07 16:26:37,039 - INFO - Batch 40/1139: Loss=7.9531, Cls=6.4672, Reg=0.0000, Con=4.9529
2025-09-07 16:26:39,854 - INFO - Batch 50/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9505
2025-09-07 16:26:42,653 - INFO - Batch 60/1139: Loss=2.1703, Cls=0.6847, Reg=0.0000, Con=4.9520
2025-09-07 16:26:45,027 - INFO - Batch 70/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9520
2025-09-07 16:26:47,720 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:26:49,920 - INFO - Batch 90/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:26:52,060 - INFO - Batch 100/1139: Loss=4.6841, Cls=3.1987, Reg=0.0000, Con=4.9511
2025-09-07 16:26:54,048 - INFO - Batch 110/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:26:56,130 - INFO - Batch 120/1139: Loss=1.5019, Cls=0.0162, Reg=0.0000, Con=4.9524
2025-09-07 16:26:58,205 - INFO - Batch 130/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:27:00,918 - INFO - Batch 140/1139: Loss=1.4871, Cls=0.0017, Reg=0.0000, Con=4.9515
2025-09-07 16:27:03,792 - INFO - Batch 150/1139: Loss=4.9694, Cls=3.4841, Reg=0.0000, Con=4.9510
2025-09-07 16:27:06,620 - INFO - Batch 160/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:27:09,462 - INFO - Batch 170/1139: Loss=1.4934, Cls=0.0081, Reg=0.0000, Con=4.9509
2025-09-07 16:27:12,245 - INFO - Batch 180/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:27:14,906 - INFO - Batch 190/1139: Loss=3.1164, Cls=1.6312, Reg=0.0000, Con=4.9506
2025-09-07 16:27:17,759 - INFO - Batch 200/1139: Loss=1.6115, Cls=0.1248, Reg=0.0000, Con=4.9556
2025-09-07 16:27:20,696 - INFO - Batch 210/1139: Loss=1.6982, Cls=0.2127, Reg=0.0000, Con=4.9517
2025-09-07 16:27:23,481 - INFO - Batch 220/1139: Loss=1.8110, Cls=0.3256, Reg=0.0000, Con=4.9512
2025-09-07 16:27:26,236 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:27:29,165 - INFO - Batch 240/1139: Loss=1.4861, Cls=0.0000, Reg=0.0000, Con=4.9535
2025-09-07 16:27:29,577 - ERROR - Error in batch 242: CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 8.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:27:29,579 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.56 GiB is allocated by PyTorch, and 8.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:27:31,909 - INFO - Batch 250/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:27:34,579 - INFO - Batch 260/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:27:37,220 - INFO - Batch 270/1139: Loss=1.4871, Cls=0.0000, Reg=0.0000, Con=4.9569
2025-09-07 16:27:39,923 - INFO - Batch 280/1139: Loss=1.5002, Cls=0.0136, Reg=0.0000, Con=4.9554
2025-09-07 16:27:42,611 - INFO - Batch 290/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:27:45,487 - INFO - Batch 300/1139: Loss=3.5088, Cls=2.0231, Reg=0.0000, Con=4.9522
2025-09-07 16:27:48,084 - INFO - Batch 310/1139: Loss=1.4851, Cls=0.0000, Reg=0.0000, Con=4.9505
2025-09-07 16:27:50,673 - INFO - Batch 320/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:27:53,459 - INFO - Batch 330/1139: Loss=1.4877, Cls=0.0020, Reg=0.0000, Con=4.9523
2025-09-07 16:27:56,232 - INFO - Batch 340/1139: Loss=3.0402, Cls=1.5548, Reg=0.0000, Con=4.9512
2025-09-07 16:27:58,891 - INFO - Batch 350/1139: Loss=1.6119, Cls=0.1268, Reg=0.0000, Con=4.9503
2025-09-07 16:28:01,726 - INFO - Batch 360/1139: Loss=3.2234, Cls=1.7371, Reg=0.0000, Con=4.9543
2025-09-07 16:28:04,378 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:28:06,957 - INFO - Batch 380/1139: Loss=1.4895, Cls=0.0041, Reg=0.0000, Con=4.9514
2025-09-07 16:28:09,599 - INFO - Batch 390/1139: Loss=1.4865, Cls=0.0000, Reg=0.0000, Con=4.9550
2025-09-07 16:28:12,283 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:28:15,034 - INFO - Batch 410/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9528
2025-09-07 16:28:17,761 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:28:20,632 - INFO - Batch 430/1139: Loss=1.4850, Cls=0.0000, Reg=0.0000, Con=4.9500
2025-09-07 16:28:23,380 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:28:26,280 - INFO - Batch 450/1139: Loss=2.6651, Cls=1.1793, Reg=0.0000, Con=4.9524
2025-09-07 16:28:28,921 - INFO - Batch 460/1139: Loss=1.4859, Cls=0.0000, Reg=0.0000, Con=4.9531
2025-09-07 16:28:31,720 - INFO - Batch 470/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9520
2025-09-07 16:28:34,506 - INFO - Batch 480/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9519
2025-09-07 16:28:37,439 - INFO - Batch 490/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:28:40,176 - INFO - Batch 500/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9519
2025-09-07 16:28:42,751 - INFO - Batch 510/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:28:45,679 - INFO - Batch 520/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9518
2025-09-07 16:28:48,515 - INFO - Batch 530/1139: Loss=1.4944, Cls=0.0085, Reg=0.0000, Con=4.9531
2025-09-07 16:28:51,373 - INFO - Batch 540/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:28:54,263 - INFO - Batch 550/1139: Loss=2.6137, Cls=1.1285, Reg=0.0000, Con=4.9505
2025-09-07 16:28:57,018 - INFO - Batch 560/1139: Loss=1.5983, Cls=0.1131, Reg=0.0000, Con=4.9508
2025-09-07 16:28:59,868 - INFO - Batch 570/1139: Loss=1.4889, Cls=0.0036, Reg=0.0000, Con=4.9508
2025-09-07 16:29:02,628 - INFO - Batch 580/1139: Loss=1.6241, Cls=0.1389, Reg=0.0000, Con=4.9507
2025-09-07 16:29:05,485 - INFO - Batch 590/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:29:08,534 - INFO - Batch 600/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9508
2025-09-07 16:29:11,168 - INFO - Batch 610/1139: Loss=2.9140, Cls=1.4298, Reg=0.0000, Con=4.9472
2025-09-07 16:29:14,111 - INFO - Batch 620/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:29:16,972 - INFO - Batch 630/1139: Loss=1.4910, Cls=0.0057, Reg=0.0000, Con=4.9508
2025-09-07 16:29:19,807 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:29:22,722 - INFO - Batch 650/1139: Loss=1.4867, Cls=0.0011, Reg=0.0000, Con=4.9520
2025-09-07 16:29:25,693 - INFO - Batch 660/1139: Loss=8.5058, Cls=7.0203, Reg=0.0000, Con=4.9517
2025-09-07 16:29:28,418 - INFO - Batch 670/1139: Loss=1.8207, Cls=1.8207, Reg=0.0000, Con=0.0000
2025-09-07 16:29:31,375 - INFO - Batch 680/1139: Loss=1.4866, Cls=0.0014, Reg=0.0000, Con=4.9506
2025-09-07 16:29:34,061 - INFO - Batch 690/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9521
2025-09-07 16:29:36,842 - INFO - Batch 700/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9524
2025-09-07 16:29:39,608 - INFO - Batch 710/1139: Loss=1.8130, Cls=0.3272, Reg=0.0000, Con=4.9528
2025-09-07 16:29:42,406 - INFO - Batch 720/1139: Loss=1.4866, Cls=0.0000, Reg=0.0000, Con=4.9552
2025-09-07 16:29:45,392 - INFO - Batch 730/1139: Loss=1.4889, Cls=0.0038, Reg=0.0000, Con=4.9503
2025-09-07 16:29:48,166 - INFO - Batch 740/1139: Loss=1.4862, Cls=0.0000, Reg=0.0000, Con=4.9539
2025-09-07 16:29:51,004 - INFO - Batch 750/1139: Loss=2.6797, Cls=1.1941, Reg=0.0000, Con=4.9518
2025-09-07 16:29:53,675 - INFO - Batch 760/1139: Loss=1.5023, Cls=0.0168, Reg=0.0000, Con=4.9518
2025-09-07 16:29:56,514 - INFO - Batch 770/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9506
2025-09-07 16:29:59,405 - INFO - Batch 780/1139: Loss=1.5143, Cls=0.0292, Reg=0.0000, Con=4.9503
2025-09-07 16:30:02,262 - INFO - Batch 790/1139: Loss=1.5416, Cls=0.0562, Reg=0.0000, Con=4.9513
2025-09-07 16:30:05,041 - INFO - Batch 800/1139: Loss=1.8875, Cls=0.4018, Reg=0.0000, Con=4.9523
2025-09-07 16:30:06,759 - ERROR - Error in batch 807: CUDA out of memory. Tried to allocate 2.62 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.20 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.08 GiB memory in use. Of the allocated memory 3.84 GiB is allocated by PyTorch, and 7.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:30:06,759 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.62 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.20 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.08 GiB memory in use. Of the allocated memory 3.84 GiB is allocated by PyTorch, and 7.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:30:07,559 - INFO - Batch 810/1139: Loss=4.2791, Cls=2.7939, Reg=0.0000, Con=4.9506
2025-09-07 16:30:10,477 - INFO - Batch 820/1139: Loss=1.5163, Cls=0.0310, Reg=0.0000, Con=4.9511
2025-09-07 16:30:13,148 - INFO - Batch 830/1139: Loss=1.4928, Cls=0.0074, Reg=0.0000, Con=4.9513
2025-09-07 16:30:16,089 - INFO - Batch 840/1139: Loss=3.2743, Cls=1.7887, Reg=0.0000, Con=4.9522
2025-09-07 16:30:19,075 - INFO - Batch 850/1139: Loss=1.4955, Cls=0.0100, Reg=0.0000, Con=4.9516
2025-09-07 16:30:21,817 - INFO - Batch 860/1139: Loss=4.6885, Cls=3.2033, Reg=0.0000, Con=4.9506
2025-09-07 16:30:24,621 - INFO - Batch 870/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:30:27,470 - INFO - Batch 880/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:30:30,252 - INFO - Batch 890/1139: Loss=1.7562, Cls=0.2709, Reg=0.0000, Con=4.9509
2025-09-07 16:30:33,054 - INFO - Batch 900/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:30:35,895 - INFO - Batch 910/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9523
2025-09-07 16:30:38,662 - INFO - Batch 920/1139: Loss=1.7077, Cls=0.2222, Reg=0.0000, Con=4.9516
2025-09-07 16:30:41,485 - INFO - Batch 930/1139: Loss=2.6624, Cls=1.1768, Reg=0.0000, Con=4.9520
2025-09-07 16:30:44,335 - INFO - Batch 940/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9519
2025-09-07 16:30:47,105 - INFO - Batch 950/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9524
2025-09-07 16:30:49,884 - INFO - Batch 960/1139: Loss=1.4896, Cls=0.0042, Reg=0.0000, Con=4.9515
2025-09-07 16:30:52,784 - INFO - Batch 970/1139: Loss=1.8001, Cls=0.3148, Reg=0.0000, Con=4.9510
2025-09-07 16:30:55,615 - INFO - Batch 980/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:30:58,568 - INFO - Batch 990/1139: Loss=3.4240, Cls=1.9383, Reg=0.0000, Con=4.9522
2025-09-07 16:31:01,316 - INFO - Batch 1000/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:31:04,153 - INFO - Batch 1010/1139: Loss=1.4859, Cls=0.0000, Reg=0.0000, Con=4.9529
2025-09-07 16:31:06,872 - INFO - Batch 1020/1139: Loss=1.6278, Cls=0.1425, Reg=0.0000, Con=4.9511
2025-09-07 16:31:09,563 - INFO - Batch 1030/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:31:12,255 - INFO - Batch 1040/1139: Loss=9.1293, Cls=7.6436, Reg=0.0000, Con=4.9524
2025-09-07 16:31:15,053 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:31:17,853 - INFO - Batch 1060/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9525
2025-09-07 16:31:20,699 - INFO - Batch 1070/1139: Loss=1.4900, Cls=0.0049, Reg=0.0000, Con=4.9501
2025-09-07 16:31:23,491 - INFO - Batch 1080/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:31:26,126 - INFO - Batch 1090/1139: Loss=1.4872, Cls=0.0018, Reg=0.0000, Con=4.9514
2025-09-07 16:31:28,847 - INFO - Batch 1100/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9505
2025-09-07 16:31:31,734 - INFO - Batch 1110/1139: Loss=1.5118, Cls=0.0270, Reg=0.0000, Con=4.9493
2025-09-07 16:31:34,423 - INFO - Batch 1120/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:31:37,238 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:34:58,934 - INFO - === Epoch 2 Results ===
2025-09-07 16:34:58,934 - INFO - Training Loss: 1.8876 (Classification: 0.4570, Regression: 0.0000)
2025-09-07 16:34:58,934 - INFO - Validation Loss: 0.1434 (Classification: 0.1434, Regression: 0.0000)
2025-09-07 16:34:58,934 - INFO - Average Classification Accuracy: 0.6146 (on 1/26 tasks)
2025-09-07 16:34:58,934 - INFO - Average AUC: 0.6485
2025-09-07 16:34:58,934 - INFO - 
Epoch 3/15
2025-09-07 16:34:59,216 - INFO - Batch 0/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9521
2025-09-07 16:35:02,101 - INFO - Batch 10/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:35:05,028 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:35:07,796 - INFO - Batch 30/1139: Loss=1.4851, Cls=0.0000, Reg=0.0000, Con=4.9504
2025-09-07 16:35:10,555 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:35:13,457 - INFO - Batch 50/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:35:16,212 - INFO - Batch 60/1139: Loss=1.4942, Cls=0.0086, Reg=0.0000, Con=4.9520
2025-09-07 16:35:19,214 - INFO - Batch 70/1139: Loss=1.5008, Cls=0.0154, Reg=0.0000, Con=4.9513
2025-09-07 16:35:21,847 - INFO - Batch 80/1139: Loss=1.4869, Cls=0.0014, Reg=0.0000, Con=4.9516
2025-09-07 16:35:24,638 - INFO - Batch 90/1139: Loss=1.5318, Cls=0.0463, Reg=0.0000, Con=4.9515
2025-09-07 16:35:27,478 - INFO - Batch 100/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:35:30,203 - INFO - Batch 110/1139: Loss=1.4907, Cls=0.0052, Reg=0.0000, Con=4.9515
2025-09-07 16:35:33,035 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:35:35,769 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:35:38,541 - INFO - Batch 140/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9525
2025-09-07 16:35:41,441 - INFO - Batch 150/1139: Loss=6.6426, Cls=5.1571, Reg=0.0000, Con=4.9515
2025-09-07 16:35:44,170 - INFO - Batch 160/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9522
2025-09-07 16:35:46,986 - INFO - Batch 170/1139: Loss=1.4906, Cls=0.0051, Reg=0.0000, Con=4.9516
2025-09-07 16:35:49,718 - INFO - Batch 180/1139: Loss=1.4985, Cls=0.0130, Reg=0.0000, Con=4.9516
2025-09-07 16:35:52,556 - INFO - Batch 190/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:35:55,314 - INFO - Batch 200/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:35:58,257 - INFO - Batch 210/1139: Loss=3.3507, Cls=1.8653, Reg=0.0000, Con=4.9514
2025-09-07 16:36:01,230 - INFO - Batch 220/1139: Loss=1.4910, Cls=0.0058, Reg=0.0000, Con=4.9505
2025-09-07 16:36:03,900 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:36:06,629 - INFO - Batch 240/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:36:09,533 - INFO - Batch 250/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:36:12,502 - INFO - Batch 260/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:36:15,231 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:36:18,225 - INFO - Batch 280/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:36:20,920 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:36:23,626 - INFO - Batch 300/1139: Loss=4.3953, Cls=2.9096, Reg=0.0000, Con=4.9524
2025-09-07 16:36:26,376 - INFO - Batch 310/1139: Loss=1.4869, Cls=0.0017, Reg=0.0000, Con=4.9508
2025-09-07 16:36:29,349 - INFO - Batch 320/1139: Loss=1.6199, Cls=0.1345, Reg=0.0000, Con=4.9510
2025-09-07 16:36:32,247 - INFO - Batch 330/1139: Loss=1.5647, Cls=0.0791, Reg=0.0000, Con=4.9520
2025-09-07 16:36:35,060 - INFO - Batch 340/1139: Loss=3.3007, Cls=3.3007, Reg=0.0000, Con=0.0000
2025-09-07 16:36:37,888 - INFO - Batch 350/1139: Loss=3.7280, Cls=2.2427, Reg=0.0000, Con=4.9510
2025-09-07 16:36:40,786 - INFO - Batch 360/1139: Loss=1.4860, Cls=0.0000, Reg=0.0000, Con=4.9534
2025-09-07 16:36:43,709 - INFO - Batch 370/1139: Loss=3.3154, Cls=1.8302, Reg=0.0000, Con=4.9507
2025-09-07 16:36:46,588 - INFO - Batch 380/1139: Loss=1.5539, Cls=0.0681, Reg=0.0000, Con=4.9526
2025-09-07 16:36:49,187 - INFO - Batch 390/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:36:51,807 - INFO - Batch 400/1139: Loss=1.4863, Cls=0.0000, Reg=0.0000, Con=4.9544
2025-09-07 16:36:54,506 - INFO - Batch 410/1139: Loss=1.5519, Cls=0.0664, Reg=0.0000, Con=4.9517
2025-09-07 16:36:57,379 - INFO - Batch 420/1139: Loss=1.5449, Cls=0.0597, Reg=0.0000, Con=4.9507
2025-09-07 16:37:00,138 - INFO - Batch 430/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:37:02,872 - INFO - Batch 440/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:37:05,653 - INFO - Batch 450/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:37:08,550 - INFO - Batch 460/1139: Loss=6.7976, Cls=5.3122, Reg=0.0000, Con=4.9512
2025-09-07 16:37:11,206 - INFO - Batch 470/1139: Loss=2.7537, Cls=1.2680, Reg=0.0000, Con=4.9523
2025-09-07 16:37:13,810 - INFO - Batch 480/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:37:16,527 - INFO - Batch 490/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9518
2025-09-07 16:37:19,154 - INFO - Batch 500/1139: Loss=1.4946, Cls=0.0092, Reg=0.0000, Con=4.9511
2025-09-07 16:37:21,844 - INFO - Batch 510/1139: Loss=1.5528, Cls=0.0673, Reg=0.0000, Con=4.9517
2025-09-07 16:37:24,294 - INFO - Batch 520/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9525
2025-09-07 16:37:27,159 - INFO - Batch 530/1139: Loss=1.4891, Cls=0.0037, Reg=0.0000, Con=4.9513
2025-09-07 16:37:29,902 - INFO - Batch 540/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:37:32,945 - INFO - Batch 550/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:37:35,853 - INFO - Batch 560/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:37:38,706 - INFO - Batch 570/1139: Loss=1.5553, Cls=0.0699, Reg=0.0000, Con=4.9515
2025-09-07 16:37:41,554 - INFO - Batch 580/1139: Loss=1.4858, Cls=0.0002, Reg=0.0000, Con=4.9520
2025-09-07 16:37:44,361 - INFO - Batch 590/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9514
2025-09-07 16:37:47,090 - INFO - Batch 600/1139: Loss=1.4864, Cls=0.0009, Reg=0.0000, Con=4.9518
2025-09-07 16:37:50,032 - INFO - Batch 610/1139: Loss=4.0505, Cls=2.5649, Reg=0.0000, Con=4.9519
2025-09-07 16:37:50,447 - ERROR - Error in batch 612: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.20 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.08 GiB memory in use. Of the allocated memory 3.74 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:37:50,447 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.20 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.08 GiB memory in use. Of the allocated memory 3.74 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:37:50,460 - ERROR - Error in batch 613: CUDA out of memory. Tried to allocate 2.63 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.21 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.07 GiB memory in use. Of the allocated memory 3.69 GiB is allocated by PyTorch, and 7.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:37:50,462 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 173, in train_epoch
    outputs = model(data, smiles_list)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/models/toxd4c.py", line 156, in forward
    atom_repr = self.geometric_encoder(atom_repr, pos, edge_index)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/models/encoders/geometric_encoder.py", line 67, in forward
    updated_x = self.propagate(edge_index, x=x, dist_emb=dist_emb)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py", line 463, in propagate
    out = self.message(**msg_kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/models/encoders/geometric_encoder.py", line 73, in message
    filter_weights = self.filter_net(dist_emb).view(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.63 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.21 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.07 GiB memory in use. Of the allocated memory 3.69 GiB is allocated by PyTorch, and 7.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:37:52,343 - INFO - Batch 620/1139: Loss=1.4854, Cls=0.0002, Reg=0.0000, Con=4.9509
2025-09-07 16:37:55,180 - INFO - Batch 630/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9523
2025-09-07 16:37:58,147 - INFO - Batch 640/1139: Loss=1.4898, Cls=0.0046, Reg=0.0000, Con=4.9508
2025-09-07 16:38:00,944 - INFO - Batch 650/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:38:03,663 - INFO - Batch 660/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:38:06,474 - INFO - Batch 670/1139: Loss=1.5316, Cls=0.0463, Reg=0.0000, Con=4.9510
2025-09-07 16:38:09,288 - INFO - Batch 680/1139: Loss=1.4879, Cls=0.0024, Reg=0.0000, Con=4.9518
2025-09-07 16:38:12,284 - INFO - Batch 690/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:38:15,109 - INFO - Batch 700/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:38:17,892 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:38:20,522 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9508
2025-09-07 16:38:23,194 - INFO - Batch 730/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:38:26,179 - INFO - Batch 740/1139: Loss=1.4863, Cls=0.0008, Reg=0.0000, Con=4.9516
2025-09-07 16:38:28,935 - INFO - Batch 750/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:38:31,894 - INFO - Batch 760/1139: Loss=6.2758, Cls=4.7906, Reg=0.0000, Con=4.9506
2025-09-07 16:38:34,822 - INFO - Batch 770/1139: Loss=1.4876, Cls=0.0023, Reg=0.0000, Con=4.9511
2025-09-07 16:38:35,457 - ERROR - Error in batch 773: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.10 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 7.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:38:35,458 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.10 GiB memory in use. Of the allocated memory 3.70 GiB is allocated by PyTorch, and 7.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:38:37,469 - INFO - Batch 780/1139: Loss=1.5217, Cls=0.0359, Reg=0.0000, Con=4.9526
2025-09-07 16:38:40,113 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:38:42,829 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:38:45,843 - INFO - Batch 810/1139: Loss=1.4870, Cls=0.0016, Reg=0.0000, Con=4.9512
2025-09-07 16:38:48,593 - INFO - Batch 820/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9512
2025-09-07 16:38:51,533 - INFO - Batch 830/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:38:54,165 - INFO - Batch 840/1139: Loss=1.4850, Cls=0.0000, Reg=0.0000, Con=4.9501
2025-09-07 16:38:57,113 - INFO - Batch 850/1139: Loss=1.4936, Cls=0.0083, Reg=0.0000, Con=4.9511
2025-09-07 16:38:59,987 - INFO - Batch 860/1139: Loss=0.0003, Cls=0.0003, Reg=0.0000, Con=0.0000
2025-09-07 16:39:02,919 - INFO - Batch 870/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9518
2025-09-07 16:39:05,562 - INFO - Batch 880/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9518
2025-09-07 16:39:08,395 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:39:11,073 - INFO - Batch 900/1139: Loss=1.4861, Cls=0.0007, Reg=0.0000, Con=4.9513
2025-09-07 16:39:13,872 - INFO - Batch 910/1139: Loss=1.4890, Cls=0.0037, Reg=0.0000, Con=4.9512
2025-09-07 16:39:16,659 - INFO - Batch 920/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9519
2025-09-07 16:39:19,357 - INFO - Batch 930/1139: Loss=1.7233, Cls=0.2380, Reg=0.0000, Con=4.9512
2025-09-07 16:39:22,171 - INFO - Batch 940/1139: Loss=1.4985, Cls=0.0133, Reg=0.0000, Con=4.9508
2025-09-07 16:39:24,847 - INFO - Batch 950/1139: Loss=1.5702, Cls=0.0848, Reg=0.0000, Con=4.9514
2025-09-07 16:39:27,731 - INFO - Batch 960/1139: Loss=6.7200, Cls=6.7200, Reg=0.0000, Con=0.0000
2025-09-07 16:39:30,504 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:39:33,275 - INFO - Batch 980/1139: Loss=1.4861, Cls=0.0007, Reg=0.0000, Con=4.9512
2025-09-07 16:39:35,941 - INFO - Batch 990/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:39:38,700 - INFO - Batch 1000/1139: Loss=1.4906, Cls=0.0053, Reg=0.0000, Con=4.9511
2025-09-07 16:39:41,585 - INFO - Batch 1010/1139: Loss=1.5859, Cls=0.1006, Reg=0.0000, Con=4.9509
2025-09-07 16:39:44,375 - INFO - Batch 1020/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9526
2025-09-07 16:39:46,995 - INFO - Batch 1030/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:39:49,501 - INFO - Batch 1040/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:39:52,384 - INFO - Batch 1050/1139: Loss=1.5443, Cls=0.0585, Reg=0.0000, Con=4.9526
2025-09-07 16:39:55,168 - INFO - Batch 1060/1139: Loss=1.4856, Cls=0.0002, Reg=0.0000, Con=4.9512
2025-09-07 16:39:57,777 - INFO - Batch 1070/1139: Loss=1.8371, Cls=0.3518, Reg=0.0000, Con=4.9510
2025-09-07 16:40:00,565 - INFO - Batch 1080/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:40:03,280 - INFO - Batch 1090/1139: Loss=7.4194, Cls=5.9338, Reg=0.0000, Con=4.9523
2025-09-07 16:40:05,949 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:40:08,930 - INFO - Batch 1110/1139: Loss=0.0149, Cls=0.0149, Reg=0.0000, Con=0.0000
2025-09-07 16:40:11,578 - INFO - Batch 1120/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9526
2025-09-07 16:40:14,351 - INFO - Batch 1130/1139: Loss=1.4863, Cls=0.0004, Reg=0.0000, Con=4.9529
2025-09-07 16:43:36,340 - INFO - === Epoch 3 Results ===
2025-09-07 16:43:36,340 - INFO - Training Loss: 1.7821 (Classification: 0.3817, Regression: 0.0000)
2025-09-07 16:43:36,340 - INFO - Validation Loss: 0.1340 (Classification: 0.1340, Regression: 0.0000)
2025-09-07 16:43:36,340 - INFO - Average Classification Accuracy: 0.6146 (on 1/26 tasks)
2025-09-07 16:43:36,340 - INFO - Average AUC: 0.6471
2025-09-07 16:43:36,340 - INFO - 
Epoch 4/15
2025-09-07 16:43:36,593 - INFO - Batch 0/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 16:43:39,420 - INFO - Batch 10/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:43:42,270 - INFO - Batch 20/1139: Loss=1.5625, Cls=0.0772, Reg=0.0000, Con=4.9511
2025-09-07 16:43:45,202 - INFO - Batch 30/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:43:47,547 - ERROR - Error in batch 39: CUDA out of memory. Tried to allocate 2.47 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 7.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:43:47,548 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.47 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.65 GiB is allocated by PyTorch, and 7.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:43:47,789 - INFO - Batch 40/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 16:43:50,523 - INFO - Batch 50/1139: Loss=4.9273, Cls=3.4414, Reg=0.0000, Con=4.9528
2025-09-07 16:43:53,466 - INFO - Batch 60/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 16:43:56,042 - INFO - Batch 70/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9525
2025-09-07 16:43:58,697 - INFO - Batch 80/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:44:01,550 - INFO - Batch 90/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:44:04,386 - INFO - Batch 100/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:44:07,092 - INFO - Batch 110/1139: Loss=1.9321, Cls=0.4466, Reg=0.0000, Con=4.9519
2025-09-07 16:44:09,840 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:44:12,287 - ERROR - Error in batch 129: CUDA out of memory. Tried to allocate 2.56 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.74 GiB is allocated by PyTorch, and 7.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:44:12,287 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.56 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.74 GiB is allocated by PyTorch, and 7.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:44:12,578 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:44:15,366 - INFO - Batch 140/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9511
2025-09-07 16:44:18,274 - INFO - Batch 150/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9512
2025-09-07 16:44:21,170 - INFO - Batch 160/1139: Loss=2.9686, Cls=1.4831, Reg=0.0000, Con=4.9516
2025-09-07 16:44:23,819 - INFO - Batch 170/1139: Loss=1.9266, Cls=0.4411, Reg=0.0000, Con=4.9516
2025-09-07 16:44:26,671 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:44:29,327 - INFO - Batch 190/1139: Loss=1.4926, Cls=0.0072, Reg=0.0000, Con=4.9514
2025-09-07 16:44:32,105 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:44:34,794 - INFO - Batch 210/1139: Loss=1.5057, Cls=0.0203, Reg=0.0000, Con=4.9514
2025-09-07 16:44:37,470 - INFO - Batch 220/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:44:40,251 - INFO - Batch 230/1139: Loss=4.3180, Cls=2.8327, Reg=0.0000, Con=4.9509
2025-09-07 16:44:43,057 - INFO - Batch 240/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:44:45,819 - INFO - Batch 250/1139: Loss=3.0702, Cls=1.5850, Reg=0.0000, Con=4.9505
2025-09-07 16:44:48,424 - INFO - Batch 260/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:44:51,069 - INFO - Batch 270/1139: Loss=1.5290, Cls=0.0437, Reg=0.0000, Con=4.9511
2025-09-07 16:44:53,475 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:44:56,275 - INFO - Batch 290/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:44:58,971 - INFO - Batch 300/1139: Loss=1.4860, Cls=0.0000, Reg=0.0000, Con=4.9532
2025-09-07 16:45:01,694 - INFO - Batch 310/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:45:04,432 - INFO - Batch 320/1139: Loss=1.4910, Cls=0.0057, Reg=0.0000, Con=4.9509
2025-09-07 16:45:07,157 - INFO - Batch 330/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:45:09,900 - INFO - Batch 340/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9514
2025-09-07 16:45:12,823 - INFO - Batch 350/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 16:45:15,880 - INFO - Batch 360/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:45:18,697 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:45:21,573 - INFO - Batch 380/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:45:24,469 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:45:27,303 - INFO - Batch 400/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:45:30,142 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:45:33,086 - INFO - Batch 420/1139: Loss=1.4861, Cls=0.0008, Reg=0.0000, Con=4.9511
2025-09-07 16:45:36,071 - INFO - Batch 430/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 16:45:38,795 - INFO - Batch 440/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9514
2025-09-07 16:45:41,567 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:45:44,282 - INFO - Batch 460/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 16:45:47,025 - INFO - Batch 470/1139: Loss=1.6163, Cls=0.1310, Reg=0.0000, Con=4.9512
2025-09-07 16:45:49,838 - INFO - Batch 480/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:45:52,612 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:45:55,370 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:45:58,152 - INFO - Batch 510/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:46:00,233 - ERROR - Error in batch 518: CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:46:00,234 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.19 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.09 GiB memory in use. Of the allocated memory 3.55 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:46:00,777 - INFO - Batch 520/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:46:03,661 - INFO - Batch 530/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:46:06,439 - INFO - Batch 540/1139: Loss=1.4890, Cls=0.0037, Reg=0.0000, Con=4.9511
2025-09-07 16:46:09,197 - INFO - Batch 550/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:46:11,996 - INFO - Batch 560/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:46:14,689 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:46:17,437 - INFO - Batch 580/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:46:20,255 - INFO - Batch 590/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9506
2025-09-07 16:46:23,136 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:46:25,771 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:46:28,650 - INFO - Batch 620/1139: Loss=1.5250, Cls=0.0396, Reg=0.0000, Con=4.9515
2025-09-07 16:46:31,595 - INFO - Batch 630/1139: Loss=1.4857, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 16:46:34,618 - INFO - Batch 640/1139: Loss=1.4912, Cls=0.0058, Reg=0.0000, Con=4.9513
2025-09-07 16:46:37,589 - INFO - Batch 650/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9521
2025-09-07 16:46:40,396 - INFO - Batch 660/1139: Loss=1.7139, Cls=0.2285, Reg=0.0000, Con=4.9514
2025-09-07 16:46:43,147 - INFO - Batch 670/1139: Loss=5.9941, Cls=4.5086, Reg=0.0000, Con=4.9517
2025-09-07 16:46:45,785 - INFO - Batch 680/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 16:46:48,412 - INFO - Batch 690/1139: Loss=1.4857, Cls=0.0002, Reg=0.0000, Con=4.9519
2025-09-07 16:46:51,213 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:46:53,716 - INFO - Batch 710/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:46:56,297 - INFO - Batch 720/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:46:59,284 - INFO - Batch 730/1139: Loss=4.9422, Cls=3.4568, Reg=0.0000, Con=4.9514
2025-09-07 16:47:02,264 - INFO - Batch 740/1139: Loss=1.5012, Cls=0.0160, Reg=0.0000, Con=4.9508
2025-09-07 16:47:05,253 - INFO - Batch 750/1139: Loss=1.5264, Cls=0.0411, Reg=0.0000, Con=4.9510
2025-09-07 16:47:08,133 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:47:11,236 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:47:14,090 - INFO - Batch 780/1139: Loss=1.4896, Cls=0.0042, Reg=0.0000, Con=4.9513
2025-09-07 16:47:16,814 - INFO - Batch 790/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:47:19,685 - INFO - Batch 800/1139: Loss=1.4866, Cls=0.0013, Reg=0.0000, Con=4.9511
2025-09-07 16:47:22,434 - INFO - Batch 810/1139: Loss=1.4862, Cls=0.0008, Reg=0.0000, Con=4.9512
2025-09-07 16:47:25,120 - INFO - Batch 820/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:47:28,008 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:47:29,751 - ERROR - Error in batch 837: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.20 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.08 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 7.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:47:29,751 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 234, in train_epoch
    total_loss_batch.backward()
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.20 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.08 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 7.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:47:29,768 - ERROR - Error in batch 838: CUDA out of memory. Tried to allocate 2.45 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.21 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.07 GiB memory in use. Of the allocated memory 3.60 GiB is allocated by PyTorch, and 8.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-09-07 16:47:29,769 - ERROR - Traceback (most recent call last):
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/train.py", line 173, in train_epoch
    outputs = model(data, smiles_list)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/models/toxd4c.py", line 156, in forward
    atom_repr = self.geometric_encoder(atom_repr, pos, edge_index)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/models/encoders/geometric_encoder.py", line 67, in forward
    updated_x = self.propagate(edge_index, x=x, dist_emb=dist_emb)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py", line 463, in propagate
    out = self.message(**msg_kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/backup2/ai4s/backupunimolpy/responds-work/ToxD4C/models/encoders/geometric_encoder.py", line 73, in message
    filter_weights = self.filter_net(dist_emb).view(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tju/miniconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.45 GiB. GPU 0 has a total capacity of 23.43 GiB of which 2.21 GiB is free. Process 197139 has 9.13 GiB memory in use. Including non-PyTorch memory, this process has 12.07 GiB memory in use. Of the allocated memory 3.60 GiB is allocated by PyTorch, and 8.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

2025-09-07 16:47:30,256 - INFO - Batch 840/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:47:32,891 - INFO - Batch 850/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9509
2025-09-07 16:47:35,368 - INFO - Batch 860/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:47:38,007 - INFO - Batch 870/1139: Loss=1.5944, Cls=0.1091, Reg=0.0000, Con=4.9509
2025-09-07 16:47:40,815 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:47:43,544 - INFO - Batch 890/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:47:46,331 - INFO - Batch 900/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:47:49,220 - INFO - Batch 910/1139: Loss=1.4897, Cls=0.0044, Reg=0.0000, Con=4.9511
2025-09-07 16:47:52,107 - INFO - Batch 920/1139: Loss=5.2657, Cls=5.2657, Reg=0.0000, Con=0.0000
2025-09-07 16:47:54,776 - INFO - Batch 930/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:47:57,489 - INFO - Batch 940/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:48:00,319 - INFO - Batch 950/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:48:03,063 - INFO - Batch 960/1139: Loss=0.0009, Cls=0.0009, Reg=0.0000, Con=0.0000
2025-09-07 16:48:05,677 - INFO - Batch 970/1139: Loss=1.4938, Cls=0.0085, Reg=0.0000, Con=4.9510
2025-09-07 16:48:08,230 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:48:10,931 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:48:13,606 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:48:16,127 - INFO - Batch 1010/1139: Loss=0.0184, Cls=0.0184, Reg=0.0000, Con=0.0000
2025-09-07 16:48:18,908 - INFO - Batch 1020/1139: Loss=1.5337, Cls=0.0484, Reg=0.0000, Con=4.9510
2025-09-07 16:48:21,588 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:48:24,385 - INFO - Batch 1040/1139: Loss=4.4219, Cls=2.9366, Reg=0.0000, Con=4.9510
2025-09-07 16:48:27,010 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:48:29,667 - INFO - Batch 1060/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:48:31,820 - INFO - Batch 1070/1139: Loss=1.4866, Cls=0.0012, Reg=0.0000, Con=4.9513
2025-09-07 16:48:33,911 - INFO - Batch 1080/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9508
2025-09-07 16:48:36,047 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:48:38,074 - INFO - Batch 1100/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:48:40,247 - INFO - Batch 1110/1139: Loss=2.2926, Cls=0.8073, Reg=0.0000, Con=4.9511
2025-09-07 16:48:42,671 - INFO - Batch 1120/1139: Loss=1.4888, Cls=0.0036, Reg=0.0000, Con=4.9508
2025-09-07 16:48:45,168 - INFO - Batch 1130/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:51:53,227 - INFO - === Epoch 4 Results ===
2025-09-07 16:51:53,227 - INFO - Training Loss: 1.7320 (Classification: 0.3226, Regression: 0.0000)
2025-09-07 16:51:53,228 - INFO - Validation Loss: 0.1596 (Classification: 0.1596, Regression: 0.0000)
2025-09-07 16:51:53,228 - INFO - Average Classification Accuracy: 0.6042 (on 1/26 tasks)
2025-09-07 16:51:53,228 - INFO - Average AUC: 0.6461
2025-09-07 16:51:53,228 - INFO - 
Epoch 5/15
2025-09-07 16:51:53,435 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:51:55,544 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:51:57,614 - INFO - Batch 20/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:51:59,639 - INFO - Batch 30/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:52:01,828 - INFO - Batch 40/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9510
2025-09-07 16:52:03,953 - INFO - Batch 50/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:52:06,079 - INFO - Batch 60/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:52:08,086 - INFO - Batch 70/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:52:10,119 - INFO - Batch 80/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:52:12,320 - INFO - Batch 90/1139: Loss=8.3798, Cls=6.8946, Reg=0.0000, Con=4.9509
2025-09-07 16:52:14,481 - INFO - Batch 100/1139: Loss=1.4880, Cls=0.0027, Reg=0.0000, Con=4.9509
2025-09-07 16:52:16,583 - INFO - Batch 110/1139: Loss=2.7464, Cls=1.2611, Reg=0.0000, Con=4.9510
2025-09-07 16:52:18,654 - INFO - Batch 120/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:52:20,775 - INFO - Batch 130/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:52:22,852 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:52:24,937 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9508
2025-09-07 16:52:27,099 - INFO - Batch 160/1139: Loss=5.2329, Cls=3.7477, Reg=0.0000, Con=4.9509
2025-09-07 16:52:29,192 - INFO - Batch 170/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:52:31,289 - INFO - Batch 180/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:52:33,453 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:52:35,559 - INFO - Batch 200/1139: Loss=1.4863, Cls=0.0010, Reg=0.0000, Con=4.9510
2025-09-07 16:52:37,627 - INFO - Batch 210/1139: Loss=1.4867, Cls=0.0013, Reg=0.0000, Con=4.9513
2025-09-07 16:52:39,834 - INFO - Batch 220/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:52:42,007 - INFO - Batch 230/1139: Loss=1.4878, Cls=0.0024, Reg=0.0000, Con=4.9512
2025-09-07 16:52:44,184 - INFO - Batch 240/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9510
2025-09-07 16:52:46,325 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:52:48,399 - INFO - Batch 260/1139: Loss=1.4862, Cls=0.0006, Reg=0.0000, Con=4.9518
2025-09-07 16:52:50,559 - INFO - Batch 270/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 16:52:52,635 - INFO - Batch 280/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:52:54,776 - INFO - Batch 290/1139: Loss=1.4857, Cls=0.0000, Reg=0.0000, Con=4.9524
2025-09-07 16:52:56,899 - INFO - Batch 300/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:52:59,083 - INFO - Batch 310/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 16:53:01,240 - INFO - Batch 320/1139: Loss=3.6160, Cls=2.1308, Reg=0.0000, Con=4.9508
2025-09-07 16:53:03,381 - INFO - Batch 330/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9515
2025-09-07 16:53:05,414 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:53:07,576 - INFO - Batch 350/1139: Loss=1.5481, Cls=0.0627, Reg=0.0000, Con=4.9511
2025-09-07 16:53:09,694 - INFO - Batch 360/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9511
2025-09-07 16:53:11,819 - INFO - Batch 370/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:53:13,912 - INFO - Batch 380/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9505
2025-09-07 16:53:16,056 - INFO - Batch 390/1139: Loss=1.4911, Cls=0.0054, Reg=0.0000, Con=4.9521
2025-09-07 16:53:18,222 - INFO - Batch 400/1139: Loss=1.4879, Cls=0.0026, Reg=0.0000, Con=4.9511
2025-09-07 16:53:20,313 - INFO - Batch 410/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:53:22,389 - INFO - Batch 420/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:53:24,594 - INFO - Batch 430/1139: Loss=4.4558, Cls=2.9705, Reg=0.0000, Con=4.9511
2025-09-07 16:53:26,749 - INFO - Batch 440/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:53:28,826 - INFO - Batch 450/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9513
2025-09-07 16:53:31,008 - INFO - Batch 460/1139: Loss=1.4918, Cls=0.0064, Reg=0.0000, Con=4.9510
2025-09-07 16:53:33,186 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:53:35,327 - INFO - Batch 480/1139: Loss=1.4861, Cls=0.0008, Reg=0.0000, Con=4.9509
2025-09-07 16:53:37,426 - INFO - Batch 490/1139: Loss=1.4860, Cls=0.0007, Reg=0.0000, Con=4.9510
2025-09-07 16:53:39,555 - INFO - Batch 500/1139: Loss=1.4856, Cls=0.0002, Reg=0.0000, Con=4.9512
2025-09-07 16:53:41,656 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:53:43,872 - INFO - Batch 520/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 16:53:46,071 - INFO - Batch 530/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9512
2025-09-07 16:53:48,263 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:53:50,342 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:53:52,462 - INFO - Batch 560/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:53:54,551 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:53:56,735 - INFO - Batch 580/1139: Loss=1.4857, Cls=0.0003, Reg=0.0000, Con=4.9513
2025-09-07 16:53:58,905 - INFO - Batch 590/1139: Loss=6.0278, Cls=4.5424, Reg=0.0000, Con=4.9512
2025-09-07 16:54:00,908 - INFO - Batch 600/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:03,062 - INFO - Batch 610/1139: Loss=0.0003, Cls=0.0003, Reg=0.0000, Con=0.0000
2025-09-07 16:54:05,226 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:54:07,337 - INFO - Batch 630/1139: Loss=3.1375, Cls=1.6521, Reg=0.0000, Con=4.9512
2025-09-07 16:54:09,442 - INFO - Batch 640/1139: Loss=3.8136, Cls=2.3283, Reg=0.0000, Con=4.9511
2025-09-07 16:54:11,520 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:54:13,626 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:54:15,775 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:54:17,864 - INFO - Batch 680/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 16:54:20,005 - INFO - Batch 690/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:22,063 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:54:24,257 - INFO - Batch 710/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:26,413 - INFO - Batch 720/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:28,471 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:54:30,576 - INFO - Batch 740/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:54:32,733 - INFO - Batch 750/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:34,860 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:54:36,929 - INFO - Batch 770/1139: Loss=1.4907, Cls=0.0052, Reg=0.0000, Con=4.9515
2025-09-07 16:54:39,056 - INFO - Batch 780/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:41,103 - INFO - Batch 790/1139: Loss=1.4848, Cls=0.0000, Reg=0.0000, Con=4.9495
2025-09-07 16:54:43,194 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:45,537 - INFO - Batch 810/1139: Loss=2.1649, Cls=0.6796, Reg=0.0000, Con=4.9509
2025-09-07 16:54:47,667 - INFO - Batch 820/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:54:49,831 - INFO - Batch 830/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:54:51,960 - INFO - Batch 840/1139: Loss=3.5977, Cls=2.1124, Reg=0.0000, Con=4.9512
2025-09-07 16:54:54,084 - INFO - Batch 850/1139: Loss=6.5555, Cls=5.0701, Reg=0.0000, Con=4.9514
2025-09-07 16:54:56,105 - INFO - Batch 860/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9518
2025-09-07 16:54:58,131 - INFO - Batch 870/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9513
2025-09-07 16:55:00,277 - INFO - Batch 880/1139: Loss=1.4870, Cls=0.0016, Reg=0.0000, Con=4.9513
2025-09-07 16:55:02,388 - INFO - Batch 890/1139: Loss=4.1773, Cls=2.6920, Reg=0.0000, Con=4.9511
2025-09-07 16:55:04,542 - INFO - Batch 900/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9510
2025-09-07 16:55:06,713 - INFO - Batch 910/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:55:08,798 - INFO - Batch 920/1139: Loss=4.8193, Cls=3.3339, Reg=0.0000, Con=4.9510
2025-09-07 16:55:10,842 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:55:12,947 - INFO - Batch 940/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:55:15,018 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:55:17,166 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:55:19,236 - INFO - Batch 970/1139: Loss=1.4867, Cls=0.0014, Reg=0.0000, Con=4.9511
2025-09-07 16:55:21,406 - INFO - Batch 980/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:55:23,573 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:55:25,597 - INFO - Batch 1000/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9513
2025-09-07 16:55:27,711 - INFO - Batch 1010/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:55:29,839 - INFO - Batch 1020/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:55:31,970 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:55:33,905 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:55:36,002 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:55:38,081 - INFO - Batch 1060/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:55:40,223 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:55:42,338 - INFO - Batch 1080/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 16:55:44,462 - INFO - Batch 1090/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 16:55:46,479 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:55:48,562 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:55:50,633 - INFO - Batch 1120/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9512
2025-09-07 16:55:52,745 - INFO - Batch 1130/1139: Loss=1.4856, Cls=0.0000, Reg=0.0000, Con=4.9519
2025-09-07 16:58:46,480 - INFO - === Epoch 5 Results ===
2025-09-07 16:58:46,480 - INFO - Training Loss: 1.6451 (Classification: 0.2432, Regression: 0.0000)
2025-09-07 16:58:46,480 - INFO - Validation Loss: 0.1809 (Classification: 0.1809, Regression: 0.0000)
2025-09-07 16:58:46,480 - INFO - Average Classification Accuracy: 0.6354 (on 1/26 tasks)
2025-09-07 16:58:46,480 - INFO - Average AUC: 0.6831
2025-09-07 16:58:46,480 - INFO - 
Epoch 6/15
2025-09-07 16:58:46,705 - INFO - Batch 0/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9510
2025-09-07 16:58:48,837 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:58:50,935 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:58:53,093 - INFO - Batch 30/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9517
2025-09-07 16:58:55,234 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:58:57,370 - INFO - Batch 50/1139: Loss=1.6230, Cls=0.1376, Reg=0.0000, Con=4.9511
2025-09-07 16:58:59,529 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:59:01,614 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:59:03,758 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 16:59:05,861 - INFO - Batch 90/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:59:07,981 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:59:10,152 - INFO - Batch 110/1139: Loss=1.4958, Cls=0.0105, Reg=0.0000, Con=4.9510
2025-09-07 16:59:12,319 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:59:14,518 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:16,671 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:18,779 - INFO - Batch 150/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:59:20,958 - INFO - Batch 160/1139: Loss=1.5015, Cls=0.0161, Reg=0.0000, Con=4.9513
2025-09-07 16:59:23,019 - INFO - Batch 170/1139: Loss=2.4584, Cls=0.9731, Reg=0.0000, Con=4.9510
2025-09-07 16:59:25,213 - INFO - Batch 180/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:59:27,394 - INFO - Batch 190/1139: Loss=1.4892, Cls=0.0038, Reg=0.0000, Con=4.9512
2025-09-07 16:59:29,505 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:59:31,637 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 16:59:33,731 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:35,973 - INFO - Batch 230/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9511
2025-09-07 16:59:38,122 - INFO - Batch 240/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:40,261 - INFO - Batch 250/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 16:59:42,469 - INFO - Batch 260/1139: Loss=3.0031, Cls=1.5178, Reg=0.0000, Con=4.9510
2025-09-07 16:59:44,562 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 16:59:46,611 - INFO - Batch 280/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 16:59:48,665 - INFO - Batch 290/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 16:59:50,773 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:52,845 - INFO - Batch 310/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:55,014 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:57,123 - INFO - Batch 330/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 16:59:59,274 - INFO - Batch 340/1139: Loss=2.5020, Cls=1.0166, Reg=0.0000, Con=4.9511
2025-09-07 17:00:01,473 - INFO - Batch 350/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9510
2025-09-07 17:00:03,587 - INFO - Batch 360/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 17:00:05,730 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:07,884 - INFO - Batch 380/1139: Loss=1.4855, Cls=0.0003, Reg=0.0000, Con=4.9508
2025-09-07 17:00:09,995 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:00:12,161 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:00:14,198 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:00:16,290 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:00:18,421 - INFO - Batch 430/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:00:20,489 - INFO - Batch 440/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:00:22,557 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:24,606 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:26,750 - INFO - Batch 470/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:00:28,790 - INFO - Batch 480/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:00:30,927 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9508
2025-09-07 17:00:33,049 - INFO - Batch 500/1139: Loss=1.4869, Cls=0.0016, Reg=0.0000, Con=4.9511
2025-09-07 17:00:35,151 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:37,309 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:39,429 - INFO - Batch 530/1139: Loss=1.4899, Cls=0.0046, Reg=0.0000, Con=4.9510
2025-09-07 17:00:41,693 - INFO - Batch 540/1139: Loss=1.6344, Cls=0.1491, Reg=0.0000, Con=4.9512
2025-09-07 17:00:43,791 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:00:46,015 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:48,176 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:00:50,245 - INFO - Batch 580/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:00:52,341 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:54,384 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:00:56,419 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:00:58,493 - INFO - Batch 620/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:01:00,602 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:02,694 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:04,828 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:06,959 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:09,042 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:11,128 - INFO - Batch 680/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 17:01:13,208 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:15,287 - INFO - Batch 700/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 17:01:17,400 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:19,430 - INFO - Batch 720/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:01:21,583 - INFO - Batch 730/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9510
2025-09-07 17:01:23,577 - INFO - Batch 740/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:01:25,731 - INFO - Batch 750/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:01:27,797 - INFO - Batch 760/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:01:29,877 - INFO - Batch 770/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:01:31,934 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:34,077 - INFO - Batch 790/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:01:36,164 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:38,326 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:40,394 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:42,522 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:44,616 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:46,768 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:01:48,952 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:51,159 - INFO - Batch 870/1139: Loss=1.5478, Cls=0.0624, Reg=0.0000, Con=4.9512
2025-09-07 17:01:53,220 - INFO - Batch 880/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:01:55,379 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:57,416 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:01:59,551 - INFO - Batch 910/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:02:01,632 - INFO - Batch 920/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:03,846 - INFO - Batch 930/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:02:05,919 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:08,074 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:02:10,080 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:02:12,237 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:14,400 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:02:16,438 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:02:18,419 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:02:20,451 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:22,560 - INFO - Batch 1020/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:02:24,676 - INFO - Batch 1030/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:02:26,752 - INFO - Batch 1040/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9512
2025-09-07 17:02:28,879 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:31,088 - INFO - Batch 1060/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9510
2025-09-07 17:02:33,221 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:35,297 - INFO - Batch 1080/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9512
2025-09-07 17:02:37,404 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:39,501 - INFO - Batch 1100/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:02:41,592 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:02:43,685 - INFO - Batch 1120/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:02:45,883 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:05:39,183 - INFO - === Epoch 6 Results ===
2025-09-07 17:05:39,183 - INFO - Training Loss: 1.5715 (Classification: 0.1591, Regression: 0.0000)
2025-09-07 17:05:39,183 - INFO - Validation Loss: 0.2140 (Classification: 0.2140, Regression: 0.0000)
2025-09-07 17:05:39,183 - INFO - Average Classification Accuracy: 0.6875 (on 1/26 tasks)
2025-09-07 17:05:39,184 - INFO - Average AUC: 0.6807
2025-09-07 17:05:39,184 - INFO - 
Epoch 7/15
2025-09-07 17:05:39,392 - INFO - Batch 0/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:05:41,529 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:05:43,646 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:05:45,694 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:05:47,734 - INFO - Batch 40/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9516
2025-09-07 17:05:49,813 - INFO - Batch 50/1139: Loss=0.0043, Cls=0.0043, Reg=0.0000, Con=0.0000
2025-09-07 17:05:51,907 - INFO - Batch 60/1139: Loss=1.4856, Cls=0.0002, Reg=0.0000, Con=4.9512
2025-09-07 17:05:54,079 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:05:56,237 - INFO - Batch 80/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9511
2025-09-07 17:05:58,325 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:06:00,496 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:06:02,651 - INFO - Batch 110/1139: Loss=1.5063, Cls=0.0209, Reg=0.0000, Con=4.9512
2025-09-07 17:06:04,754 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:06,894 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:09,052 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:06:11,193 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:06:13,274 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:06:15,310 - INFO - Batch 170/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:17,398 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:06:19,396 - INFO - Batch 190/1139: Loss=1.4898, Cls=0.0045, Reg=0.0000, Con=4.9510
2025-09-07 17:06:21,382 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:23,484 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:25,538 - INFO - Batch 220/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:06:27,506 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:06:29,456 - INFO - Batch 240/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:06:31,481 - INFO - Batch 250/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:33,451 - INFO - Batch 260/1139: Loss=1.4851, Cls=0.0000, Reg=0.0000, Con=4.9504
2025-09-07 17:06:35,454 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:06:37,501 - INFO - Batch 280/1139: Loss=1.5041, Cls=0.0187, Reg=0.0000, Con=4.9511
2025-09-07 17:06:39,474 - INFO - Batch 290/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9509
2025-09-07 17:06:41,455 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:43,357 - INFO - Batch 310/1139: Loss=1.4855, Cls=0.0000, Reg=0.0000, Con=4.9515
2025-09-07 17:06:45,377 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:47,339 - INFO - Batch 330/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:06:49,325 - INFO - Batch 340/1139: Loss=3.4128, Cls=1.9274, Reg=0.0000, Con=4.9512
2025-09-07 17:06:51,290 - INFO - Batch 350/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 17:06:53,294 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:55,281 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:06:57,288 - INFO - Batch 380/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:06:59,282 - INFO - Batch 390/1139: Loss=1.4874, Cls=0.0021, Reg=0.0000, Con=4.9510
2025-09-07 17:07:01,288 - INFO - Batch 400/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:07:03,334 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:05,311 - INFO - Batch 420/1139: Loss=1.4925, Cls=0.0071, Reg=0.0000, Con=4.9511
2025-09-07 17:07:07,315 - INFO - Batch 430/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:07:09,297 - INFO - Batch 440/1139: Loss=1.4857, Cls=0.0003, Reg=0.0000, Con=4.9512
2025-09-07 17:07:11,296 - INFO - Batch 450/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:07:13,215 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:15,269 - INFO - Batch 470/1139: Loss=8.0318, Cls=6.5464, Reg=0.0000, Con=4.9511
2025-09-07 17:07:17,237 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:07:19,256 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:21,244 - INFO - Batch 500/1139: Loss=0.0769, Cls=0.0769, Reg=0.0000, Con=0.0000
2025-09-07 17:07:23,217 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:25,188 - INFO - Batch 520/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:07:27,177 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:29,188 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:31,197 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:33,211 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:35,235 - INFO - Batch 570/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:07:37,224 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:39,291 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:41,434 - INFO - Batch 600/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9512
2025-09-07 17:07:43,493 - INFO - Batch 610/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:07:45,651 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:47,605 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:49,776 - INFO - Batch 640/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:07:51,934 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:07:54,091 - INFO - Batch 660/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:07:56,198 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:07:58,334 - INFO - Batch 680/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:08:00,398 - INFO - Batch 690/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:08:02,491 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:04,636 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:06,649 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:08,814 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:10,926 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:13,083 - INFO - Batch 750/1139: Loss=7.9919, Cls=6.5066, Reg=0.0000, Con=4.9511
2025-09-07 17:08:15,111 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:17,254 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:19,380 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:21,568 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:23,637 - INFO - Batch 800/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:08:25,771 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:27,896 - INFO - Batch 820/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:08:29,991 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:32,093 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:34,266 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:36,288 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:38,414 - INFO - Batch 870/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9510
2025-09-07 17:08:40,516 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:42,625 - INFO - Batch 890/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:44,691 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:46,794 - INFO - Batch 910/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:08:48,955 - INFO - Batch 920/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:51,158 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:08:53,290 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:08:55,434 - INFO - Batch 950/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:08:57,530 - INFO - Batch 960/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:08:59,672 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:09:01,711 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:09:03,883 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:09:05,910 - INFO - Batch 1000/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:09:08,070 - INFO - Batch 1010/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:09:10,147 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:09:12,299 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:09:14,470 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:09:16,564 - INFO - Batch 1050/1139: Loss=1.4913, Cls=0.0060, Reg=0.0000, Con=4.9511
2025-09-07 17:09:18,737 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:09:20,907 - INFO - Batch 1070/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:09:23,003 - INFO - Batch 1080/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:09:25,098 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:09:27,219 - INFO - Batch 1100/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9513
2025-09-07 17:09:29,263 - INFO - Batch 1110/1139: Loss=1.8780, Cls=0.3925, Reg=0.0000, Con=4.9514
2025-09-07 17:09:31,409 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:09:33,449 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:12:27,142 - INFO - === Epoch 7 Results ===
2025-09-07 17:12:27,143 - INFO - Training Loss: 1.5804 (Classification: 0.1655, Regression: 0.0000)
2025-09-07 17:12:27,143 - INFO - Validation Loss: 0.2135 (Classification: 0.2135, Regression: 0.0000)
2025-09-07 17:12:27,143 - INFO - Average Classification Accuracy: 0.6562 (on 1/26 tasks)
2025-09-07 17:12:27,143 - INFO - Average AUC: 0.6862
2025-09-07 17:12:27,143 - INFO - 
Epoch 8/15
2025-09-07 17:12:27,362 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:12:29,551 - INFO - Batch 10/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:12:31,730 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:12:33,819 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:12:35,806 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:12:37,904 - INFO - Batch 50/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:12:40,036 - INFO - Batch 60/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9511
2025-09-07 17:12:42,117 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:12:44,281 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:12:46,412 - INFO - Batch 90/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:12:48,620 - INFO - Batch 100/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9512
2025-09-07 17:12:50,629 - INFO - Batch 110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:12:52,775 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:12:54,919 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:12:57,106 - INFO - Batch 140/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:12:59,285 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:01,399 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:03,475 - INFO - Batch 170/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:05,667 - INFO - Batch 180/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:13:07,699 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:09,724 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:11,872 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:14,026 - INFO - Batch 220/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:13:16,168 - INFO - Batch 230/1139: Loss=1.5946, Cls=0.1092, Reg=0.0000, Con=4.9511
2025-09-07 17:13:18,329 - INFO - Batch 240/1139: Loss=3.1939, Cls=1.7086, Reg=0.0000, Con=4.9511
2025-09-07 17:13:20,566 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:13:22,636 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:24,742 - INFO - Batch 270/1139: Loss=5.8918, Cls=4.4065, Reg=0.0000, Con=4.9510
2025-09-07 17:13:26,833 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:28,842 - INFO - Batch 290/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:13:30,852 - INFO - Batch 300/1139: Loss=1.8626, Cls=0.3773, Reg=0.0000, Con=4.9511
2025-09-07 17:13:33,057 - INFO - Batch 310/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9510
2025-09-07 17:13:35,160 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:37,387 - INFO - Batch 330/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9511
2025-09-07 17:13:39,520 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:41,588 - INFO - Batch 350/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:43,693 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:45,802 - INFO - Batch 370/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:13:47,917 - INFO - Batch 380/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:13:50,081 - INFO - Batch 390/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:13:52,166 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:13:54,324 - INFO - Batch 410/1139: Loss=2.0407, Cls=0.5554, Reg=0.0000, Con=4.9511
2025-09-07 17:13:56,399 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:13:58,521 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:00,560 - INFO - Batch 440/1139: Loss=0.0725, Cls=0.0725, Reg=0.0000, Con=0.0000
2025-09-07 17:14:02,627 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:14:04,742 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:06,790 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:08,977 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:11,111 - INFO - Batch 490/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:14:13,318 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:15,377 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:17,563 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:19,767 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:21,844 - INFO - Batch 540/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:14:23,938 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:14:26,037 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:28,110 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:30,253 - INFO - Batch 580/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9511
2025-09-07 17:14:32,331 - INFO - Batch 590/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:14:34,484 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:36,521 - INFO - Batch 610/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:14:38,644 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:40,806 - INFO - Batch 630/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:14:42,960 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:45,009 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:47,050 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:49,224 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:14:51,397 - INFO - Batch 680/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:53,513 - INFO - Batch 690/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:14:55,690 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:14:57,847 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:00,025 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:02,111 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:04,285 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:06,313 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:08,476 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:10,545 - INFO - Batch 770/1139: Loss=6.5871, Cls=5.1017, Reg=0.0000, Con=4.9512
2025-09-07 17:15:12,678 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:14,784 - INFO - Batch 790/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:15:16,828 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:18,938 - INFO - Batch 810/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:15:21,083 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:23,154 - INFO - Batch 830/1139: Loss=6.5255, Cls=5.0402, Reg=0.0000, Con=4.9510
2025-09-07 17:15:25,238 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:27,338 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:29,518 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:31,679 - INFO - Batch 870/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9511
2025-09-07 17:15:33,854 - INFO - Batch 880/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9511
2025-09-07 17:15:35,885 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:37,978 - INFO - Batch 900/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:15:40,128 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:42,222 - INFO - Batch 920/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:44,238 - INFO - Batch 930/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9512
2025-09-07 17:15:46,440 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:48,547 - INFO - Batch 950/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:50,640 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:52,717 - INFO - Batch 970/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:15:54,792 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:15:56,909 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:15:59,109 - INFO - Batch 1000/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:16:01,270 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:16:03,373 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:16:05,543 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:16:07,711 - INFO - Batch 1040/1139: Loss=1.5257, Cls=0.0404, Reg=0.0000, Con=4.9511
2025-09-07 17:16:09,804 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:16:11,973 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:16:14,110 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:16:16,182 - INFO - Batch 1080/1139: Loss=11.7379, Cls=10.2526, Reg=0.0000, Con=4.9510
2025-09-07 17:16:18,251 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:16:20,367 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:16:22,569 - INFO - Batch 1110/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:16:24,682 - INFO - Batch 1120/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:16:26,876 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:19:20,518 - INFO - === Epoch 8 Results ===
2025-09-07 17:19:20,518 - INFO - Training Loss: 1.5735 (Classification: 0.1560, Regression: 0.0000)
2025-09-07 17:19:20,518 - INFO - Validation Loss: 0.2092 (Classification: 0.2092, Regression: 0.0000)
2025-09-07 17:19:20,518 - INFO - Average Classification Accuracy: 0.6562 (on 1/26 tasks)
2025-09-07 17:19:20,518 - INFO - Average AUC: 0.7092
2025-09-07 17:19:20,518 - INFO - 
Epoch 9/15
2025-09-07 17:19:20,743 - INFO - Batch 0/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:19:22,824 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:19:24,920 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:19:27,072 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:29,127 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:31,248 - INFO - Batch 50/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:33,415 - INFO - Batch 60/1139: Loss=1.4869, Cls=0.0015, Reg=0.0000, Con=4.9512
2025-09-07 17:19:35,496 - INFO - Batch 70/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:19:37,453 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:39,477 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:41,459 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:43,476 - INFO - Batch 110/1139: Loss=1.4901, Cls=0.0048, Reg=0.0000, Con=4.9510
2025-09-07 17:19:45,485 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:19:47,527 - INFO - Batch 130/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9510
2025-09-07 17:19:49,654 - INFO - Batch 140/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9510
2025-09-07 17:19:51,727 - INFO - Batch 150/1139: Loss=3.6106, Cls=2.1253, Reg=0.0000, Con=4.9511
2025-09-07 17:19:53,876 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:19:55,963 - INFO - Batch 170/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:19:58,093 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:00,222 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:02,258 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:04,389 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:06,467 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:08,560 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:10,592 - INFO - Batch 240/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:12,678 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:14,859 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:16,973 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:19,108 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:21,202 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:23,322 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:25,402 - INFO - Batch 310/1139: Loss=2.8412, Cls=1.3558, Reg=0.0000, Con=4.9511
2025-09-07 17:20:27,535 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:29,640 - INFO - Batch 330/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:20:31,788 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:33,872 - INFO - Batch 350/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:35,995 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:38,125 - INFO - Batch 370/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:20:40,294 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:42,446 - INFO - Batch 390/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:20:44,529 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:46,716 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:48,774 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:50,937 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:53,084 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:55,168 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:20:57,219 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:20:59,386 - INFO - Batch 470/1139: Loss=1.4911, Cls=0.0057, Reg=0.0000, Con=4.9511
2025-09-07 17:21:01,439 - INFO - Batch 480/1139: Loss=2.4228, Cls=0.9374, Reg=0.0000, Con=4.9511
2025-09-07 17:21:03,487 - INFO - Batch 490/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:21:05,607 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:07,706 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:09,899 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:12,005 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:14,113 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:16,239 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:18,363 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:20,447 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:22,534 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:24,486 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:26,665 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:28,652 - INFO - Batch 610/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:21:30,787 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:32,975 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:35,178 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:37,275 - INFO - Batch 650/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9511
2025-09-07 17:21:39,478 - INFO - Batch 660/1139: Loss=1.5008, Cls=0.0155, Reg=0.0000, Con=4.9510
2025-09-07 17:21:41,628 - INFO - Batch 670/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:21:43,816 - INFO - Batch 680/1139: Loss=1.4865, Cls=0.0012, Reg=0.0000, Con=4.9511
2025-09-07 17:21:45,900 - INFO - Batch 690/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9511
2025-09-07 17:21:47,930 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:49,975 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:21:52,153 - INFO - Batch 720/1139: Loss=0.0019, Cls=0.0019, Reg=0.0000, Con=0.0000
2025-09-07 17:21:54,236 - INFO - Batch 730/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:21:56,335 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:21:58,484 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:00,603 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:02,684 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:04,848 - INFO - Batch 780/1139: Loss=1.4858, Cls=0.0005, Reg=0.0000, Con=4.9510
2025-09-07 17:22:06,972 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:09,071 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:22:11,129 - INFO - Batch 810/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:22:13,181 - INFO - Batch 820/1139: Loss=1.4873, Cls=0.0020, Reg=0.0000, Con=4.9510
2025-09-07 17:22:15,297 - INFO - Batch 830/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:22:17,386 - INFO - Batch 840/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:22:19,557 - INFO - Batch 850/1139: Loss=1.4879, Cls=0.0026, Reg=0.0000, Con=4.9511
2025-09-07 17:22:21,561 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:23,616 - INFO - Batch 870/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:22:25,747 - INFO - Batch 880/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:22:27,824 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:22:29,956 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:32,092 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:34,306 - INFO - Batch 920/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9512
2025-09-07 17:22:36,438 - INFO - Batch 930/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9510
2025-09-07 17:22:38,611 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:22:40,691 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:22:42,844 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:45,179 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:47,285 - INFO - Batch 980/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:22:49,449 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:51,537 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:53,530 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:55,745 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:57,823 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:22:59,941 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:23:02,040 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:23:04,092 - INFO - Batch 1060/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9514
2025-09-07 17:23:06,239 - INFO - Batch 1070/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:23:08,378 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:23:10,571 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:23:12,720 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:23:14,836 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:23:16,976 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:23:19,093 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:12,957 - INFO - === Epoch 9 Results ===
2025-09-07 17:26:12,958 - INFO - Training Loss: 1.5104 (Classification: 0.0994, Regression: 0.0000)
2025-09-07 17:26:12,958 - INFO - Validation Loss: 0.2725 (Classification: 0.2725, Regression: 0.0000)
2025-09-07 17:26:12,958 - INFO - Average Classification Accuracy: 0.6250 (on 1/26 tasks)
2025-09-07 17:26:12,958 - INFO - Average AUC: 0.6523
2025-09-07 17:26:12,958 - INFO - 
Epoch 10/15
2025-09-07 17:26:13,162 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:15,334 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:17,412 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:19,583 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:21,830 - INFO - Batch 40/1139: Loss=1.9234, Cls=0.4381, Reg=0.0000, Con=4.9511
2025-09-07 17:26:24,011 - INFO - Batch 50/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:26,098 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:28,176 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:30,325 - INFO - Batch 80/1139: Loss=1.4867, Cls=0.0014, Reg=0.0000, Con=4.9510
2025-09-07 17:26:32,427 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:34,550 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:36,684 - INFO - Batch 110/1139: Loss=1.5534, Cls=0.0680, Reg=0.0000, Con=4.9510
2025-09-07 17:26:38,814 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:40,886 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:43,003 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:45,127 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:47,181 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:49,286 - INFO - Batch 170/1139: Loss=1.4865, Cls=0.0012, Reg=0.0000, Con=4.9511
2025-09-07 17:26:51,321 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:26:53,468 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:26:55,526 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:57,578 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:26:59,705 - INFO - Batch 220/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9510
2025-09-07 17:27:01,862 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:04,039 - INFO - Batch 240/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:06,221 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:08,318 - INFO - Batch 260/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:27:10,517 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:12,542 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9509
2025-09-07 17:27:14,731 - INFO - Batch 290/1139: Loss=1.4887, Cls=0.0036, Reg=0.0000, Con=4.9506
2025-09-07 17:27:16,749 - INFO - Batch 300/1139: Loss=1.4858, Cls=0.0000, Reg=0.0000, Con=4.9525
2025-09-07 17:27:18,914 - INFO - Batch 310/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:21,013 - INFO - Batch 320/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:27:23,112 - INFO - Batch 330/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:25,184 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:27,339 - INFO - Batch 350/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9513
2025-09-07 17:27:29,472 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:31,519 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:33,643 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:35,708 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:37,822 - INFO - Batch 400/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:27:39,971 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:42,100 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:44,251 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:46,409 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:48,497 - INFO - Batch 450/1139: Loss=1.6329, Cls=0.1475, Reg=0.0000, Con=4.9511
2025-09-07 17:27:50,592 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:27:52,696 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:54,843 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:56,979 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:27:59,148 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:01,235 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:03,398 - INFO - Batch 520/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:28:05,472 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:07,557 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:09,642 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:11,829 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:13,987 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:16,056 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:18,200 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:20,334 - INFO - Batch 600/1139: Loss=1.4857, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:28:22,370 - INFO - Batch 610/1139: Loss=0.0009, Cls=0.0009, Reg=0.0000, Con=0.0000
2025-09-07 17:28:24,688 - INFO - Batch 620/1139: Loss=1.5009, Cls=0.0156, Reg=0.0000, Con=4.9511
2025-09-07 17:28:26,878 - INFO - Batch 630/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:28:29,018 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:31,123 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:33,222 - INFO - Batch 660/1139: Loss=1.4855, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:28:35,260 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:37,386 - INFO - Batch 680/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:39,615 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:41,793 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:43,867 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:45,960 - INFO - Batch 720/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:28:48,037 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:50,120 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:28:52,157 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:54,327 - INFO - Batch 760/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9511
2025-09-07 17:28:56,533 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:28:58,710 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:29:00,841 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:02,940 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:05,003 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:07,127 - INFO - Batch 820/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:29:09,280 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:11,350 - INFO - Batch 840/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:29:13,531 - INFO - Batch 850/1139: Loss=1.4857, Cls=0.0004, Reg=0.0000, Con=4.9510
2025-09-07 17:29:15,541 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:17,652 - INFO - Batch 870/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:19,818 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:29:21,943 - INFO - Batch 890/1139: Loss=2.3100, Cls=0.8247, Reg=0.0000, Con=4.9510
2025-09-07 17:29:24,016 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:26,129 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:28,216 - INFO - Batch 920/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:30,359 - INFO - Batch 930/1139: Loss=1.5005, Cls=0.0152, Reg=0.0000, Con=4.9510
2025-09-07 17:29:32,557 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:34,691 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:29:36,874 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:38,979 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:41,100 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:43,122 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:45,220 - INFO - Batch 1000/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:47,373 - INFO - Batch 1010/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:29:49,524 - INFO - Batch 1020/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:29:51,648 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:29:53,779 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:55,899 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:29:58,012 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:30:00,113 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9512
2025-09-07 17:30:02,186 - INFO - Batch 1080/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:30:04,340 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:30:06,460 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:30:08,546 - INFO - Batch 1110/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:30:10,638 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:30:12,648 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:05,767 - INFO - === Epoch 10 Results ===
2025-09-07 17:33:05,768 - INFO - Training Loss: 1.4989 (Classification: 0.0723, Regression: 0.0000)
2025-09-07 17:33:05,768 - INFO - Validation Loss: 0.2904 (Classification: 0.2904, Regression: 0.0000)
2025-09-07 17:33:05,768 - INFO - Average Classification Accuracy: 0.6146 (on 1/26 tasks)
2025-09-07 17:33:05,768 - INFO - Average AUC: 0.6497
2025-09-07 17:33:07,326 - INFO - Checkpoint saved to: experiments/r1c3_single_cls_0_seed_42_20250907_161748/checkpoints/r1c3_single_cls_0_seed_42_epoch_10.pth
2025-09-07 17:33:07,326 - INFO - 
Epoch 11/15
2025-09-07 17:33:07,548 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:09,707 - INFO - Batch 10/1139: Loss=1.4990, Cls=0.0136, Reg=0.0000, Con=4.9511
2025-09-07 17:33:11,834 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:13,863 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:33:15,952 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:18,074 - INFO - Batch 50/1139: Loss=1.4863, Cls=0.0010, Reg=0.0000, Con=4.9512
2025-09-07 17:33:20,205 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:22,306 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:24,491 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:26,656 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:28,785 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:30,901 - INFO - Batch 110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:33:33,023 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:33:35,132 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:33:37,298 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:39,502 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:41,599 - INFO - Batch 160/1139: Loss=1.4852, Cls=0.0000, Reg=0.0000, Con=4.9507
2025-09-07 17:33:43,664 - INFO - Batch 170/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9510
2025-09-07 17:33:45,794 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:47,933 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:50,042 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:52,176 - INFO - Batch 210/1139: Loss=1.4861, Cls=0.0008, Reg=0.0000, Con=4.9510
2025-09-07 17:33:54,336 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:56,459 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:33:58,675 - INFO - Batch 240/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:00,854 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:03,082 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:05,215 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:07,358 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:09,499 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:34:11,614 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:34:13,675 - INFO - Batch 310/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:34:15,805 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:17,956 - INFO - Batch 330/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:34:19,964 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:34:22,141 - INFO - Batch 350/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:24,318 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:26,368 - INFO - Batch 370/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:34:28,469 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:30,621 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:32,737 - INFO - Batch 400/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9510
2025-09-07 17:34:34,857 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:37,014 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:39,082 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:41,203 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:43,260 - INFO - Batch 450/1139: Loss=2.2828, Cls=0.7974, Reg=0.0000, Con=4.9510
2025-09-07 17:34:45,343 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:47,394 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:49,479 - INFO - Batch 480/1139: Loss=1.4875, Cls=0.0022, Reg=0.0000, Con=4.9510
2025-09-07 17:34:51,666 - INFO - Batch 490/1139: Loss=1.4888, Cls=0.0035, Reg=0.0000, Con=4.9510
2025-09-07 17:34:53,762 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:55,877 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:34:57,937 - INFO - Batch 520/1139: Loss=1.4997, Cls=0.0143, Reg=0.0000, Con=4.9512
2025-09-07 17:35:00,024 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:02,194 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:04,280 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:06,406 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:35:08,545 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:35:10,679 - INFO - Batch 580/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:35:12,704 - INFO - Batch 590/1139: Loss=1.6559, Cls=0.1706, Reg=0.0000, Con=4.9511
2025-09-07 17:35:14,782 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:16,940 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:19,190 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:21,293 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:35:23,455 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:25,575 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:35:27,705 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:29,754 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:31,929 - INFO - Batch 680/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:35:34,065 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:36,198 - INFO - Batch 700/1139: Loss=1.6898, Cls=0.2044, Reg=0.0000, Con=4.9511
2025-09-07 17:35:38,401 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:40,500 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:42,690 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:44,909 - INFO - Batch 740/1139: Loss=3.3605, Cls=1.8751, Reg=0.0000, Con=4.9511
2025-09-07 17:35:47,036 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:49,139 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:35:51,273 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:53,412 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:55,548 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:35:57,687 - INFO - Batch 800/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:35:59,746 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:01,879 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:04,000 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:06,121 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:08,282 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:10,366 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:12,490 - INFO - Batch 870/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:36:14,612 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:16,793 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:18,908 - INFO - Batch 900/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:36:20,985 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:23,163 - INFO - Batch 920/1139: Loss=1.4859, Cls=0.0006, Reg=0.0000, Con=4.9511
2025-09-07 17:36:25,367 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:27,427 - INFO - Batch 940/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:36:29,511 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:31,673 - INFO - Batch 960/1139: Loss=1.4861, Cls=0.0008, Reg=0.0000, Con=4.9511
2025-09-07 17:36:33,747 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:35,869 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:38,085 - INFO - Batch 990/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:40,120 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:42,270 - INFO - Batch 1010/1139: Loss=1.4857, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:36:44,432 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:46,643 - INFO - Batch 1030/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:36:48,652 - INFO - Batch 1040/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:36:50,785 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:52,837 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:55,002 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:36:57,090 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:36:59,239 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:37:01,471 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:37:03,577 - INFO - Batch 1110/1139: Loss=0.2281, Cls=0.2281, Reg=0.0000, Con=0.0000
2025-09-07 17:37:05,718 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:37:07,952 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:02,159 - INFO - === Epoch 11 Results ===
2025-09-07 17:40:02,159 - INFO - Training Loss: 1.5066 (Classification: 0.0760, Regression: 0.0000)
2025-09-07 17:40:02,159 - INFO - Validation Loss: 0.3199 (Classification: 0.3199, Regression: 0.0000)
2025-09-07 17:40:02,159 - INFO - Average Classification Accuracy: 0.5938 (on 1/26 tasks)
2025-09-07 17:40:02,159 - INFO - Average AUC: 0.6594
2025-09-07 17:40:02,159 - INFO - 
Epoch 12/15
2025-09-07 17:40:02,366 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:04,419 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:06,545 - INFO - Batch 20/1139: Loss=1.5214, Cls=0.0361, Reg=0.0000, Con=4.9511
2025-09-07 17:40:08,615 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:10,649 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:12,777 - INFO - Batch 50/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:14,900 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:16,939 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:19,005 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:21,149 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:23,312 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:25,397 - INFO - Batch 110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:27,587 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:29,776 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:31,941 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:34,073 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:36,288 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:38,421 - INFO - Batch 170/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:40,562 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:42,636 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:44,817 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:46,861 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:48,959 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:51,117 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:53,263 - INFO - Batch 240/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:40:55,404 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:57,531 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:40:59,615 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:01,668 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:03,761 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:05,819 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:07,946 - INFO - Batch 310/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:10,056 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:12,174 - INFO - Batch 330/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:14,314 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:16,401 - INFO - Batch 350/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:18,564 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:20,705 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:22,832 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:24,925 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:26,978 - INFO - Batch 400/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:41:29,170 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:31,306 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:33,517 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:35,700 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:37,858 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:39,908 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:42,020 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:44,227 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:46,372 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:48,458 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:50,584 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:41:52,640 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:54,607 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:56,714 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:41:58,805 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:00,935 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:03,110 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:05,252 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:07,270 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:09,419 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:11,479 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:42:13,494 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:42:15,573 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:17,730 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:19,837 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:21,999 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:42:24,140 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:26,187 - INFO - Batch 680/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:28,389 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:30,534 - INFO - Batch 700/1139: Loss=1.4861, Cls=0.0008, Reg=0.0000, Con=4.9511
2025-09-07 17:42:32,639 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:34,736 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:36,916 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:42:38,922 - INFO - Batch 740/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:42:41,057 - INFO - Batch 750/1139: Loss=1.4858, Cls=0.0004, Reg=0.0000, Con=4.9510
2025-09-07 17:42:43,215 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:45,214 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:47,340 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:49,333 - INFO - Batch 790/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:42:51,445 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:42:53,564 - INFO - Batch 810/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:42:55,692 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:57,885 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:42:59,933 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:43:02,051 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:04,226 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:43:06,379 - INFO - Batch 870/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:43:08,518 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:43:10,667 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:12,766 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:14,794 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:16,876 - INFO - Batch 920/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9510
2025-09-07 17:43:18,984 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:21,104 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:23,252 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:43:25,314 - INFO - Batch 960/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:43:27,444 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:29,642 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:31,791 - INFO - Batch 990/1139: Loss=1.9368, Cls=0.4515, Reg=0.0000, Con=4.9511
2025-09-07 17:43:33,851 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:35,903 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:38,123 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:40,261 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:42,391 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:44,555 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:46,676 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:48,746 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:50,898 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:52,937 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:54,966 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:57,073 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:43:59,157 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:44:01,235 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:46:54,555 - INFO - === Epoch 12 Results ===
2025-09-07 17:46:54,555 - INFO - Training Loss: 1.4576 (Classification: 0.0558, Regression: 0.0000)
2025-09-07 17:46:54,555 - INFO - Validation Loss: 0.3310 (Classification: 0.3310, Regression: 0.0000)
2025-09-07 17:46:54,555 - INFO - Average Classification Accuracy: 0.6250 (on 1/26 tasks)
2025-09-07 17:46:54,555 - INFO - Average AUC: 0.6546
2025-09-07 17:46:54,555 - INFO - 
Epoch 13/15
2025-09-07 17:46:54,756 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:46:56,872 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:46:59,008 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:01,099 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:03,211 - INFO - Batch 40/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:47:05,288 - INFO - Batch 50/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:47:07,254 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:09,268 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:11,444 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:13,570 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:15,740 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:17,941 - INFO - Batch 110/1139: Loss=2.3652, Cls=0.8799, Reg=0.0000, Con=4.9511
2025-09-07 17:47:20,137 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:22,228 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:24,394 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:26,454 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:28,571 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:30,748 - INFO - Batch 170/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:32,869 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:35,057 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:37,144 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:39,272 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:41,386 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:43,485 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:45,606 - INFO - Batch 240/1139: Loss=1.5096, Cls=0.0243, Reg=0.0000, Con=4.9510
2025-09-07 17:47:47,695 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:49,817 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:51,909 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:53,992 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:47:56,030 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:47:58,123 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:00,199 - INFO - Batch 310/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:02,233 - INFO - Batch 320/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:48:04,436 - INFO - Batch 330/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:06,604 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:08,790 - INFO - Batch 350/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:10,937 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:13,086 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:15,165 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:17,328 - INFO - Batch 390/1139: Loss=1.6222, Cls=0.1369, Reg=0.0000, Con=4.9511
2025-09-07 17:48:19,427 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:21,580 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:23,713 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:25,852 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:27,969 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:30,083 - INFO - Batch 450/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:48:32,158 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:34,309 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:36,409 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:38,454 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:40,564 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:42,719 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:44,821 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:46,910 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:48:49,063 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:51,189 - INFO - Batch 550/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:53,232 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:55,394 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:48:57,577 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:48:59,605 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:01,597 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:03,640 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:05,697 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:07,804 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:09,942 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:12,054 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:14,237 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:16,352 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:18,374 - INFO - Batch 680/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:49:20,548 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:22,575 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:24,709 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:26,781 - INFO - Batch 720/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:28,999 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:31,115 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:33,241 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:35,426 - INFO - Batch 760/1139: Loss=1.4856, Cls=0.0003, Reg=0.0000, Con=4.9511
2025-09-07 17:49:37,552 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:39,711 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:41,822 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:43,890 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:45,989 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:48,081 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:50,279 - INFO - Batch 830/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:49:52,383 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:54,572 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:49:56,670 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:49:58,814 - INFO - Batch 870/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:50:00,897 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:50:03,077 - INFO - Batch 890/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:05,197 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:07,354 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:09,435 - INFO - Batch 920/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:11,590 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:50:13,695 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:15,773 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:17,957 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:20,137 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:22,251 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:24,426 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:26,571 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:28,713 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:50:30,866 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:50:33,023 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:35,154 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:37,284 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:39,476 - INFO - Batch 1060/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 17:50:41,576 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:50:43,887 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:46,030 - INFO - Batch 1090/1139: Loss=3.4353, Cls=1.9500, Reg=0.0000, Con=4.9511
2025-09-07 17:50:48,077 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:50,168 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:50:52,259 - INFO - Batch 1120/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:50:54,428 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:53:47,818 - INFO - === Epoch 13 Results ===
2025-09-07 17:53:47,818 - INFO - Training Loss: 1.4830 (Classification: 0.0616, Regression: 0.0000)
2025-09-07 17:53:47,818 - INFO - Validation Loss: 0.3207 (Classification: 0.3207, Regression: 0.0000)
2025-09-07 17:53:47,818 - INFO - Average Classification Accuracy: 0.6250 (on 1/26 tasks)
2025-09-07 17:53:47,819 - INFO - Average AUC: 0.6490
2025-09-07 17:53:47,819 - INFO - 
Epoch 14/15
2025-09-07 17:53:48,046 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:53:50,158 - INFO - Batch 10/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:53:52,266 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:53:54,363 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:53:56,490 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:53:58,600 - INFO - Batch 50/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:00,719 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:02,774 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:04,810 - INFO - Batch 80/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:06,965 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:09,149 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:11,295 - INFO - Batch 110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:13,403 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:15,508 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:17,620 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:19,713 - INFO - Batch 150/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:54:21,915 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:24,044 - INFO - Batch 170/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:26,203 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:28,427 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:30,528 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:32,699 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:34,790 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:36,938 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:39,055 - INFO - Batch 240/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:41,098 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:43,332 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:45,440 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:47,628 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:49,874 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:51,906 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:54:54,033 - INFO - Batch 310/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:54:56,110 - INFO - Batch 320/1139: Loss=1.4875, Cls=0.0022, Reg=0.0000, Con=4.9511
2025-09-07 17:54:58,233 - INFO - Batch 330/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:00,356 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:02,515 - INFO - Batch 350/1139: Loss=0.0668, Cls=0.0668, Reg=0.0000, Con=0.0000
2025-09-07 17:55:04,607 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:55:06,722 - INFO - Batch 370/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:08,874 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:55:10,972 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:13,097 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:55:15,206 - INFO - Batch 410/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:55:17,360 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:55:19,393 - INFO - Batch 430/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:21,506 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:23,641 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:25,779 - INFO - Batch 460/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 17:55:27,899 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:30,048 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:32,148 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:34,192 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:55:36,164 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:38,276 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:40,431 - INFO - Batch 530/1139: Loss=1.5097, Cls=0.0244, Reg=0.0000, Con=4.9511
2025-09-07 17:55:42,556 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:44,652 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:46,732 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:48,867 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:51,093 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:53,245 - INFO - Batch 590/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:55,407 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:57,503 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:55:59,602 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:01,766 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:03,896 - INFO - Batch 640/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:05,998 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:08,146 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:10,249 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:12,434 - INFO - Batch 680/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:56:14,523 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:56:16,580 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:18,896 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:21,038 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:23,174 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:25,323 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:56:27,475 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:29,578 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:31,701 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:33,796 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:35,880 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:37,948 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:40,065 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:42,051 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:44,234 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:46,313 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:48,407 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:50,492 - INFO - Batch 860/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:52,598 - INFO - Batch 870/1139: Loss=4.3391, Cls=2.8538, Reg=0.0000, Con=4.9510
2025-09-07 17:56:54,751 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:56,821 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:56:58,982 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:57:01,093 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:03,191 - INFO - Batch 920/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 17:57:05,299 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:07,321 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:09,444 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:11,592 - INFO - Batch 960/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:13,732 - INFO - Batch 970/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:15,836 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:17,929 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:57:20,078 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:22,178 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:24,332 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:57:26,400 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:28,529 - INFO - Batch 1040/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:57:30,627 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:32,725 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:34,860 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:37,008 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:39,133 - INFO - Batch 1090/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:41,250 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:43,477 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 17:57:45,547 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 17:57:47,608 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:40,832 - INFO - === Epoch 14 Results ===
2025-09-07 18:00:40,833 - INFO - Training Loss: 1.4525 (Classification: 0.0441, Regression: 0.0000)
2025-09-07 18:00:40,833 - INFO - Validation Loss: 0.3410 (Classification: 0.3410, Regression: 0.0000)
2025-09-07 18:00:40,833 - INFO - Average Classification Accuracy: 0.6250 (on 1/26 tasks)
2025-09-07 18:00:40,833 - INFO - Average AUC: 0.6544
2025-09-07 18:00:40,833 - INFO - 
Epoch 15/15
2025-09-07 18:00:41,038 - INFO - Batch 0/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:43,216 - INFO - Batch 10/1139: Loss=1.4865, Cls=0.0012, Reg=0.0000, Con=4.9510
2025-09-07 18:00:45,380 - INFO - Batch 20/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:47,506 - INFO - Batch 30/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:49,616 - INFO - Batch 40/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:51,790 - INFO - Batch 50/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:53,931 - INFO - Batch 60/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:56,145 - INFO - Batch 70/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:00:58,264 - INFO - Batch 80/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 18:01:00,316 - INFO - Batch 90/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:02,335 - INFO - Batch 100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:04,486 - INFO - Batch 110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:06,642 - INFO - Batch 120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:08,913 - INFO - Batch 130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:10,970 - INFO - Batch 140/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:01:13,084 - INFO - Batch 150/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:15,138 - INFO - Batch 160/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:17,259 - INFO - Batch 170/1139: Loss=1.4854, Cls=0.0001, Reg=0.0000, Con=4.9511
2025-09-07 18:01:19,340 - INFO - Batch 180/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:21,399 - INFO - Batch 190/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:23,372 - INFO - Batch 200/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:25,438 - INFO - Batch 210/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:27,554 - INFO - Batch 220/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:29,649 - INFO - Batch 230/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:31,719 - INFO - Batch 240/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:33,821 - INFO - Batch 250/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:35,937 - INFO - Batch 260/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:38,009 - INFO - Batch 270/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:40,064 - INFO - Batch 280/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:42,112 - INFO - Batch 290/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:01:44,149 - INFO - Batch 300/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:46,163 - INFO - Batch 310/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 18:01:48,293 - INFO - Batch 320/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:50,299 - INFO - Batch 330/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:52,369 - INFO - Batch 340/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:54,529 - INFO - Batch 350/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:56,548 - INFO - Batch 360/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:01:58,651 - INFO - Batch 370/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 18:02:00,776 - INFO - Batch 380/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:02,948 - INFO - Batch 390/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:05,066 - INFO - Batch 400/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:07,173 - INFO - Batch 410/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:09,281 - INFO - Batch 420/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:11,419 - INFO - Batch 430/1139: Loss=1.4868, Cls=0.0015, Reg=0.0000, Con=4.9511
2025-09-07 18:02:13,441 - INFO - Batch 440/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:15,466 - INFO - Batch 450/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:17,521 - INFO - Batch 460/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:19,630 - INFO - Batch 470/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:21,729 - INFO - Batch 480/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:23,887 - INFO - Batch 490/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:25,963 - INFO - Batch 500/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:28,014 - INFO - Batch 510/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:30,099 - INFO - Batch 520/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:32,084 - INFO - Batch 530/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:34,191 - INFO - Batch 540/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:36,306 - INFO - Batch 550/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:38,378 - INFO - Batch 560/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:40,365 - INFO - Batch 570/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:42,468 - INFO - Batch 580/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:44,543 - INFO - Batch 590/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:46,649 - INFO - Batch 600/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:48,702 - INFO - Batch 610/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:50,858 - INFO - Batch 620/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:52,942 - INFO - Batch 630/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:02:54,985 - INFO - Batch 640/1139: Loss=1.4855, Cls=0.0002, Reg=0.0000, Con=4.9511
2025-09-07 18:02:57,152 - INFO - Batch 650/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:02:59,298 - INFO - Batch 660/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:03:01,315 - INFO - Batch 670/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:03,428 - INFO - Batch 680/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:05,585 - INFO - Batch 690/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:07,712 - INFO - Batch 700/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:03:09,840 - INFO - Batch 710/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:11,873 - INFO - Batch 720/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:14,039 - INFO - Batch 730/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:16,100 - INFO - Batch 740/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:03:18,172 - INFO - Batch 750/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:20,268 - INFO - Batch 760/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:03:22,298 - INFO - Batch 770/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:24,481 - INFO - Batch 780/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:26,541 - INFO - Batch 790/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:28,585 - INFO - Batch 800/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:30,579 - INFO - Batch 810/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:03:32,565 - INFO - Batch 820/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:34,585 - INFO - Batch 830/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:36,534 - INFO - Batch 840/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:03:38,549 - INFO - Batch 850/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:40,564 - INFO - Batch 860/1139: Loss=1.4854, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:42,512 - INFO - Batch 870/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:44,572 - INFO - Batch 880/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:46,580 - INFO - Batch 890/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:48,478 - INFO - Batch 900/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:50,433 - INFO - Batch 910/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:52,342 - INFO - Batch 920/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:54,326 - INFO - Batch 930/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:56,291 - INFO - Batch 940/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:03:58,310 - INFO - Batch 950/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:04:00,276 - INFO - Batch 960/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 18:04:02,196 - INFO - Batch 970/1139: Loss=0.0000, Cls=0.0000, Reg=0.0000, Con=0.0000
2025-09-07 18:04:04,177 - INFO - Batch 980/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:06,142 - INFO - Batch 990/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:08,206 - INFO - Batch 1000/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:10,133 - INFO - Batch 1010/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:04:12,161 - INFO - Batch 1020/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:14,175 - INFO - Batch 1030/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:16,225 - INFO - Batch 1040/1139: Loss=0.0351, Cls=0.0351, Reg=0.0000, Con=0.0000
2025-09-07 18:04:18,206 - INFO - Batch 1050/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:20,158 - INFO - Batch 1060/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:22,247 - INFO - Batch 1070/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:24,378 - INFO - Batch 1080/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:26,472 - INFO - Batch 1090/1139: Loss=1.5356, Cls=0.0503, Reg=0.0000, Con=4.9511
2025-09-07 18:04:28,512 - INFO - Batch 1100/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9510
2025-09-07 18:04:30,500 - INFO - Batch 1110/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:32,607 - INFO - Batch 1120/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:04:34,622 - INFO - Batch 1130/1139: Loss=1.4853, Cls=0.0000, Reg=0.0000, Con=4.9511
2025-09-07 18:07:27,354 - INFO - === Epoch 15 Results ===
2025-09-07 18:07:27,354 - INFO - Training Loss: 1.4364 (Classification: 0.0216, Regression: 0.0000)
2025-09-07 18:07:27,354 - INFO - Validation Loss: 0.3486 (Classification: 0.3486, Regression: 0.0000)
2025-09-07 18:07:27,354 - INFO - Average Classification Accuracy: 0.6042 (on 1/26 tasks)
2025-09-07 18:07:27,354 - INFO - Average AUC: 0.6471
2025-09-07 18:07:27,354 - INFO - Training finished!
2025-09-07 18:07:27,354 - INFO - Performing final evaluation...
2025-09-07 18:07:28,028 - INFO - Loaded best model from: experiments/r1c3_single_cls_0_seed_42_20250907_161748/checkpoints/r1c3_single_cls_0_seed_42_best.pth
2025-09-07 18:10:18,209 - INFO - Final Validation Results:
2025-09-07 18:10:18,209 - INFO -   Total Loss: 0.1014
2025-09-07 18:10:18,209 - INFO -   Classification Loss: 0.1014
2025-09-07 18:10:18,209 - INFO -   Regression Loss: 0.0000
2025-09-07 18:10:18,209 - INFO -   Average Classification Accuracy: 0.5521
2025-09-07 18:10:18,209 - INFO -   Average AUC: 0.5954
2025-09-07 18:10:18,210 - INFO - Results saved to: experiments/r1c3_single_cls_0_seed_42_20250907_161748/checkpoints/r1c3_single_cls_0_seed_42_results.json
